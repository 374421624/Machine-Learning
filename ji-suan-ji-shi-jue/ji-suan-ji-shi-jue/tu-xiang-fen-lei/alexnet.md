# AlexNet

AlexNet可以说是一个具有里程碑意义的网络结构。在2012年的ILSVRC 2012中一举刷新了纪录，Top-5错误率比上一年的冠军下降了十多个百分点，而且远远超过当年的第二名ISI，从而奠定了深度学习在计算机视觉领域的霸主地位。

AlexNet网络包含8个学习层：5个卷积层和3个全连接层，最后的输出层为一个1000类的Softmax层。AlexNet模型中间层分为两路，明确显示了两块GPU之间的职责划分——一块GPU运行图中顶部模型部分，另一块GPU则运行图中底部模型部分。GPU之间仅在某些层相互通信。该网络的输入是150528维的，且该网络剩下的各层的神经元数分别为253440-186624-64896-64896-43264-4096-1000。

![](../../../.gitbook/assets/hoxiq.png)

## AlexNet结构

结构如下图所示，5个卷积阶段（这里我不称为卷积层是因为这5部分不仅进行了卷积操作，还有其他运算），再接3个全连接层，最后1000维的Softmax输出。每层的维度及核数量如下下图所示。

![](../../../.gitbook/assets/timline-jie-tu-20190114160500.png)

![](../../../.gitbook/assets/timline-jie-tu-20190114154959.png)

### 第一层\(卷积\)

![](../../../.gitbook/assets/20170516212848372.png)

第一层输入数据为227\*227\*3（原始为224\*224\*3，加了padding，为了保证卷积之后图像尺寸依然是整数）的图像，这个图像被11\*11\*3的卷积核进行卷积运算，卷积核对原始图像的每次卷积都生成一个新的像素。卷积核沿原始图像的 $$x$$ 轴方向和 $$y$$ 轴方向两个方向移动，移动的步长是4个像素。因此，卷积核在移动的过程中会生成 $$(227-11)/4+1=55$$ 个像素\(227个像素减去11，正好是54，即生成54个像素，再加上被减去的11也对应生成一个像素\)，行和列的55\*55个像素形成对原始图像卷积之后的像素层。共有96个卷积核，会生成55\*55\*96个卷积后的像素层。96个卷积核分成2组，每组48个卷积核。对应生成2组55\*55\*48的卷积后的像素层数据。这些像素层经过ReLU单元的处理，生成激活像素层，尺寸仍为2组55\*55\*48的像素层数据。

这些像素层经过池化处理，池化的尺寸为3\*3，运算的步长为2，则池化有图像的尺寸为 $$(55-3)/2+1=27$$ 。即池化后像素的规模为27\*27\*96；然后经过归一化处理，归一化运算的尺度为5\*5；第一卷积层运算结束后形成的像素层的规模为27\*27\*96。分别对应96个卷积核所运算形成。这96层像素层分为2组，每组48个像素层，每组在一个独立的GPU上进行运算。

反向传播时，每个卷积核对应一个偏差值。即第一层的96个卷积核对应上层输入的96个偏差值。

### 第二层\(卷积\)

![](../../../.gitbook/assets/20170516212902357.png)

第二层输入数据为第一层输出的27\*27\*96的像素层，为便于后续处理，每幅像素层的左右两边和上下两边都要填充2个像素；27\*27\*96的像素数据分成27\*27\*48的两组像素数据，两组数据分别再两个不同的GPU中进行运算。每组像素数据被 $$5*5*48$$ 的卷积核进行卷积运算，卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿原始图像的 $$x$$ 轴方向和 $$y$$ 轴方向两个方向移动，移动的步长是1个像素。因此，卷积核在移动的过程中会生成 $$(27-5+2*2)/1+1=27$$ 个像素\(27个像素减去5，正好是22，在加上上下、左右各填充的2个像素，即生成26个像素，再加上被减去的5也对应生成一个像素\)。行和列的27\*27个像素形成对原始图像卷积之后的像素层。共有256个5\*5\*48卷积核；这256个卷积核分成2组，每组针对一个GPU中的27\*27\*48的像素进行卷积运算。会生成27\*27\*128个卷积后的像素层。这些像素层经过ReLU单元的处理，生成激活像素层，尺寸仍为2组27\*27\*256的像素层。

这些像素层经过池化运算，池化的尺寸为3\*3，运算的步长为2，则池化后的图像尺寸为 $$(27-3)/2+1=13$$ 。即池化后像素的规模为2组13\*13\*128的像素层；然后经过归一化处理，归一化运算尺寸为5\*5；第二卷积层运算结束后形成的像素层的规模为2组13\*13\*128的像素层。分别对应2组128个卷积核所运算形成。每组在一个GPU上进行运算。即共256个卷积核，共2个GPU进行运算。

反向传播时，每个卷积核对应一个偏差值。即第一层的96个卷积核对应上层输入的256个偏差值。

### 第三层\(卷积\)

![](../../../.gitbook/assets/20170516212921278%20%281%29.png)

第三层输入数据为第二层输出的2组13\*13\*128的像素层；为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有192个卷积核，每个卷积核的尺寸是3\*3\*256.因此，每个GPU中的卷积核都能对2组13\*13\*128的像素层的所有数据进行卷积运算。卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿像素层数据的 $$x$$ 轴方向和 $$y$$ 轴方向两个方向移动，移动的步长是1个像素。因此，运算后的卷积核的尺寸为 $$(13-3+1*2)/1+1=13$$ （13个像素减去3，正好是10，在加上上下、左右各填充的1个像素，即生成12个像素，再加上被减去的3也对应生成一个像素），每个GPU中共13\*13\*192个卷积核。2个GPU中共13\*13\*384个卷积后的像素层。这些像素层经过ReLU单元的处理，生成激活像素层，尺寸仍为2组13\*13\*192像素层，共13\*13\*384个像素层。

### 第四层\(卷积\)

![](../../../.gitbook/assets/20170516213035624.bin)

第四层输入数据为第三层输出的2组13\*13\*192的像素层；为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有192个卷积核，每个卷积核的尺寸是3\*3\*192。因此，每个GPU中的卷积核能对1组13\*13\*192的像素层的数据进行卷积运算。卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿像素层数据的 $$x$$ 轴方向和 $$y$$ 轴方向两个方向移动，移动的步长是1个像素。因此，运算后的卷积核的尺寸为 $$(13-3+1*2)/1+1=13$$ （13个像素减去3，正好是10，在加上上下、左右各填充的1个像素，即生成12个像素，再加上被减去的3也对应生成一个像素），每个GPU中共13\*13\*192个卷积核。2个GPU中共13\*13\*384个卷积后的像素层。这些像素层经过ReLU单元的处理，生成激活像素层，尺寸仍为2组13\*13\*192像素层，共13\*13\*384个像素层。

### 第五层\(卷积\)

![](../../../.gitbook/assets/20170516213050279.bin)

第五层输入数据为第四层输出的2组13\*13\*192的像素层；为便于后续处理，每幅像素层的左右两边和上下两边都要填充1个像素；2组像素层数据都被送至2个不同的GPU中进行运算。每个GPU中都有128个卷积核，每个卷积核的尺寸是3\*3\*192。因此，每个GPU中的卷积核能对1组13\*13\*192的像素层的数据进行卷积运算。卷积核对每组数据的每次卷积都生成一个新的像素。卷积核沿像素层数据的 $$x$$ 轴方向和 $$y$$ 轴方向两个方向移动，移动的步长是1个像素。因此，运算后的卷积核的尺寸为 $$(13-3+1*2)/1+1=13$$ （13个像素减去3，正好是10，在加上上下、左右各填充的1个像素，即生成12个像素，再加上被减去的3也对应生成一个像素），每个GPU中共13\*13\*128个卷积核。2个GPU中共13\*13\*256个卷积后的像素层。这些像素层经过ReLU单元的处理，生成激活像素层，尺寸仍为2组13\*13\*128像素层，共13\*13\*256个像素层。

2组13\*13\*128像素层分别在2个不同GPU中进行池化运算。池化运算的尺寸为3\*3，运算步长为2，则池化后图像的尺寸为 $$(13-3)/2+1=6$$ 。即池化后像素的规模为2组6\*6\*128的像素层数据，共6\*6\*256规模的像素层数据。

### 第六层\(全连接\)

![](../../../.gitbook/assets/20170516213056139.bin)

第六层输入数据的尺寸是6\*6\*256，采用6\*6\*256尺寸的滤波器对第六层的输入数据进行卷积运算；每个6\*6\*256尺寸的滤波器对第六层的输入数据进行卷积运算生成一个运算结果，通过一个神经元输出这个运算结果；共有4096个6\*6\*256尺寸的滤波器对输入数据进行卷积运算，通过4096个神经元输出运算结果；这4096个运算结果通过ReLU激活函数生成4096个值；并通过drop运算后输出4096个本层的输出结果值。

由于第六层的运算过程中，采用的滤波器的尺寸6\*6\*256与待处理的特征图的尺寸相同，即滤波器中的每个系数只与特征图中的一个像素值相乘；而其它卷积层中，每个滤波器的系数都会与多个特征图中像素值相乘；因此，将第六层称为全连接层。

第五层输出的6\*6\*256规模的像素层数据与第六层的4096个神经元进行全连接，然后经由ReLU进行处理后生成4096个数据，再经过dropout处理后输出4096个数据。

### 第七层\(全连接\)

![](../../../.gitbook/assets/20170516213102123.bin)

第六层输出的4096个数据与第七层的4096个神经元进行全连接，然后经由ReLU进行处理后生成4096个数据，再经过dropout处理后输出4096个数据。

### 第八层\(全连接\)

![](../../../.gitbook/assets/20170516213108672.bin)

第七层输出的4096个数据与第八层的1000个神经元进行全连接，经过训练后输出被训练的数值。

## AlexNet特点

![](../../../.gitbook/assets/20170516213118686.bin)

### ReLU非线性激活函数

### 百万数据和多GPU训练

### 局部响应归一化

### 重叠池化

### 整体网络结构

### 降低过拟合

## Source

{% embed url="http://vision.stanford.edu/teaching/cs231b\_spring1415/slides/alexnet\_tugce\_kyunghee.pdf" %}

{% embed url="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" %}

{% embed url="https://blog.csdn.net/zyqdragon/article/details/72353420" %}





