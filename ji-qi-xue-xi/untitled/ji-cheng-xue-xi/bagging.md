# Bagging

Bagging是指的个体学习器间不存在强依赖关系、可同时生成的并行化方法。

Bagging基于自助采样法，给定包含 $$m$$ 个样本的数据集，我们随机取出一个样本放入采样集中，再把该样本放回初始数据集，使得下次采样时该样本仍有可能被选中，这样，经过 $$m$$ 次操作，我们得到含 $$m$$ 个样本的采样集，初始训练集里有的样本在采样集里多次出现，有的则从未出现。由 $$\lim\limits_{m\to\infty}(1-\frac{1}{m})^m\to\frac{1}{e}\approx0.368$$ 可知初始训练集约有 $$63.2\%$$ 的样本出现在采样集中， $$36.8\%$$ 未出现在采样集中。

照这样，我们可采集出 $$T$$ 个含 $$m$$ 个训练样本的训练集，然后基于每个采样集训练出一个基学习器，再将这些学习器进行结合。Bagging通常对分类任务使用简单投票法，对回归任务使用简单平均法。

值得一提的是，自助采样过程还给Bagging带来了另一个有点：由于每个基学习器只使用了初始训练集中约 $$63.2\%$$ 的样本，剩下的样本可用作验证集来对泛化性能进行“包外估计”。

## 随机森林\(Random Forest\)

随机森林在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。具体来说，在随机森林中，对基决策树的每个结点，先从该结点的属性集合中随机选择一个包含 $$k$$ 个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里的参数 $$k$$ 控制了随机性的引入程度：若 $$k = d$$ ，则基决策树的构建与传统决策树相同；若令 $$k = 1$$ ，则是随机选择一个属性用于划分；一般情况下，推荐值 $$k = \log_2d$$ 。

随机森林的训练效率常常优于纯Bagging思想的决策树群，因为在个体决策树的构建过程中，Bagging使用的是“确定型”决策树，在选择划分属性时要对结点的所有属性进行考察，而随即森林使用的“随机型”决策树则只考察一个属性子集。

