# XGBoost

## XGBoost简介 

在数据建模中，经常采用Boosting方法，该方法将成百上千个分类准确率较低的树模型组合起来，成为一个准确率很高的预测模型。这个模型会不断地迭代，每次迭代就生成一颗新的树。但在数据集较复杂的时候，可能需要几千次迭代运算，这将造成巨大的计算瓶颈。

针对这个问题。华盛顿大学的陈天奇博士开发的XGBoost（eXtreme Gradient Boosting）基于C++通过多线程实现了回归树的并行构建，并在原有Gradient Boosting算法基础上加以改进，从而极大地提升了模型训练速度和预测精度。在Kaggle的比赛中，多次有队伍借助XGBoost在比赛中夺得第一。其次，因为它的效果好，计算复杂度不高，在工业界中也有大量的应用。

## 监督学习的三要素

因为Boosting Tree本身是一种有监督学习算法，要讲Boosting Tree，先从监督学习讲起。在监督学习中有几个逻辑上的重要组成部件，粗略地可以分为：模型、参数、目标函数和优化算法。

### 模型

模型指的是给定输入 $$x_i$$ 如何去预测输出 $$y_i$$ 。我们比较常见的模型如线性模型（包括线性回归和Logistic Regression）采用线性加和的方式进行预测

                                                                    $$\hat{y_i}=\sum\limits_jw_j x_{ij}$$ 

这里的预测值 $$y$$ 可以有不同的解释，比如我们可以把它作为回归目标的输出，或者进行sigmoid变换得到概率（即用 $$\frac{1}{1+e^{-\hat{y_i}}}$$ 来预测正例的概率），或者作为排序的指标等。一个线性模型根据 $$y$$ 的解释不同（以及设计对应的目标函数），分别用到回归、分类或者排序等场景。

### 参数

参数就是我们根据模型要从数据里头学习的东西，比如线性模型中的线性系数：

                                                        $$\theta_j=\{w_j|j=1,2,\dots,d\}$$ 

### 目标函数：误差函数+正则化项

模型和参数本身指定了给定输入我们如何预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数函数登场了。一般地目标函数包含两项：一项是损失函数，它说明了我们的模型有多拟合数据；另一项是正则化项，它惩罚了复杂模型。

                                                           $$\text{Obj}(\theta)=L(\theta)+\Omega(\theta)$$ 

1、 $$L(\theta)$$：损失函数 $$L=\sum\limits_{i=1}^n(y_i,\hat{y}_i)$$ ，常见的损失函数有：

* 平方损失： $$l(y_i,\hat{y}_i)=(y_i-\hat{y}_i)^2$$ 
* Logistic损失： $$l(y_i,\hat{y}_i)=y_i\ln (1+e^{-\hat{y}_i})+(1-y_i)\ln(1+e^{\hat{y}_i})$$ 

2、 $$\Omega(\theta)$$ ：正则化项，之所以要引入它是因为我们的目标是希望生成的模型能准确地预测新的样本（即应用于测试数据集），而不是简单地拟合训练集的结果（这样会导致过拟合）。所以需要在保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能。而正则化项就是用于惩罚复杂模型，避免模型过分拟合训练数据。常用的正则有 $$L^1$$ 正则与 $$L^2$$ 正则：

* $$L^1$$ 正则\(Lasso\)： $$\Omega(w)=\lambda||w||_1$$ 
* $$L^2$$ 正则： $$\Omega(w)=\lambda||w||^2$$ 

上述目标函数的设计来自于统计学习里面的一个重要概念叫做Bias-variance tradeoff（偏差-方差权衡），比较感性的理解，Bias可以理解为假设我们有无限多数据的时候，可以训练出最好的模型所拿到的误差。而Variance是因为我们只有有限数据，其中随机性带来的误差。目标中误差函数鼓励我们的模型尽量去拟合训练数据，这样相对来说最后的模型会有比较少的Bias。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。

## XGBoost原理

### 目标函数及二阶泰勒展开

在XGBoost中，选择树模型为基学习器，我们需要正则的对象，或者说需要控制复杂度的对象就是这 $$K$$ 颗树,通常树的参数有树的深度，叶子节点的个数，叶子节点值的取值（XGBoost里称为权重weight\)。所以，我们的目标函数形式如下：

                                                        $$\text{Obj}(\theta)=\sum\limits_{i=1}^nL(y_i,\hat{y}_i)+\sum\limits_{i=k}^K\Omega(f_k)$$ 

其中第一部分是训练损失，如上面所述的平方损失或者Logistic Loss等，第二部分是每棵树的复杂度的和。因为现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式。即每次迭代生成一棵新的回归树，从而使预测值不断逼近真实值（即进一步最小化目标函数）。每一次保留原来的模型不变，加入一个新的函数 $$f$$ 到模型里面：

                                                                           $$\hat{y}_i^0=c$$ （ $$c$$ 为常数）

                                                                      $$\hat{y}_i^1=\hat{y}_i^0+f_1(x_i)$$ 

                                                                      $$\hat{y}_i^2=\hat{y}_i^1+f_2(x_i)$$ 

                                                                   $$\hat{y}_i^K=\hat{y}_i^{K-1}+f_K(x_i)$$ 

所以，对于第 $$K$$ 次的目标函数为：

                                                           $$\text{Obj}^K=\sum\limits_iL(y_i,\hat{y}_i^K)+\Omega(f_k)+c$$ 

其中 $$c$$ 为前 $$K-1$$ 棵树的正则化项和，是一个常数。

根据 $$\hat{y}_i^K=\hat{y}_i^{K-1}+f_K(x_i)$$ ，进一步为：

                                                 $$\text{Obj}^K=\sum\limits_iL(y_i,\hat{y}_i^{K-1}+f_K(x_i))+\Omega(f_K)+c$$ 

上面的式子意思很明显，只需要寻找一棵合适的树 $$f_K$$ 使得目标函数最小。然后不断地迭代 $$K$$ 次就可以完成 $$K$$ 个学习器的训练。那么我们这棵树到底怎么找呢？在GBDT里面（当然GBDT没有正则），我们的树是通过拟合上一棵树的负梯度值，建树的时候采用的启发式准则，如MSE等。然而，在XGBoost里面，它是通过对损失函数进行二阶泰勒展开：

                                                 $$f(x+\Delta x)=f(x)+f'(x)\Delta x+\frac{1}{2}f''(x)\Delta x^2$$ 

对损失函数二阶泰勒展开：

  $$\sum\limits_iL(y_i,\hat{y}_i^{K-1}+f_K(x_i))=\sum\limits_i[L(y_i,\hat{y}_i^{K-1})+L'(y_i,\hat{y}_i^{K-1})f_K(x_i)+\frac{1}{2}L''(y_i,\hat{y}_i^{K-1})f^2_K(x_i)]$$

注意的是，这里的 $$y_i$$ 是标签值，是个常数，而 $$\hat{y}_i^{K-1}$$ 是第 $$K-1$$ 次学习的结果，也是个常数。所以只要把变化量 $$\Delta x$$ 看成我们需要学习的模型 $$f_K(x)$$ ，把 $$\hat{y}_i^{K-1}$$ 看作 $$x$$ 就可以展成上面的样子。

这里，我们用 $$g_i$$ 记为第 $$i$$ 个样本损失函数的一阶导， $$h_i$$ 记为第 $$i$$ 个样本损失函数的二阶导。

                                                                   $$g_i = L'(y_i,\hat{y}_i^{K-1})$$ 

                                                                   $$h_i = L''(y_i,\hat{y}_i^{K-1})$$ 

上面两式非常重要，贯串整个树的创建（分裂，叶子节点值的计算）。同时二阶导式子是我们利用XGBoost做特征选择时的一个评价指标。

### 分裂准则

### 算法流程

XGBoost核心部分的算法流程图如下：

![](../../../../.gitbook/assets/20180310232620896.png)

## Source

{% embed url="https://blog.csdn.net/anshuai\_aw1/article/details/82970489\#\_604" %}

{% embed url="https://blog.csdn.net/anshuai\_aw1/article/details/85093106" %}



