# XGBoost

## XGBoost简介 

在数据建模中，经常采用Boosting方法，该方法将成百上千个分类准确率较低的树模型组合起来，成为一个准确率很高的预测模型。这个模型会不断地迭代，每次迭代就生成一颗新的树。但在数据集较复杂的时候，可能需要几千次迭代运算，这将造成巨大的计算瓶颈。

针对这个问题。华盛顿大学的陈天奇博士开发的XGBoost（eXtreme Gradient Boosting）基于C++通过多线程实现了回归树的并行构建，并在原有Gradient Boosting算法基础上加以改进，从而极大地提升了模型训练速度和预测精度。在Kaggle的比赛中，多次有队伍借助XGBoost在比赛中夺得第一。其次，因为它的效果好，计算复杂度不高，在工业界中也有大量的应用。

## 监督学习的三要素

因为Boosting Tree本身是一种有监督学习算法，要讲Boosting Tree，先从监督学习讲起。在监督学习中有几个逻辑上的重要组成部件，粗略地可以分为：模型、参数、目标函数和优化算法。

### 模型

模型指的是给定输入 $$x_i$$ 如何去预测输出 $$y_i$$ 。我们比较常见的模型如线性模型（包括线性回归和Logistic Regression）采用线性加和的方式进行预测

                                                                    $$\hat{y_i}=\sum\limits_jw_j x_{ij}$$ 

这里的预测值 $$y$$ 可以有不同的解释，比如我们可以把它作为回归目标的输出，或者进行sigmoid变换得到概率（即用 $$\frac{1}{1+e^{-\hat{y_i}}}$$ 来预测正例的概率），或者作为排序的指标等。一个线性模型根据 $$y$$ 的解释不同（以及设计对应的目标函数），分别用到回归、分类或者排序等场景。

### 参数

参数就是我们根据模型要从数据里头学习的东西，比如线性模型中的线性系数：

                                                        $$\theta_j=\{w_j|j=1,2,\dots,d\}$$ 

### 目标函数：误差函数+正则化项

模型和参数本身指定了给定输入我们如何预测，但是没有告诉我们如何去寻找一个比较好的参数，这个时候就需要目标函数函数登场了。一般地目标函数包含两项：一项是损失函数，它说明了我们的模型有多拟合数据；另一项是正则化项，它惩罚了复杂模型。

                                                           $$\text{Obj}(\theta)=L(\theta)+\Omega(\theta)$$ 

1、 $$L(\theta)$$：损失函数 $$L=\sum\limits_{i=1}^n(y_i,\hat{y}_i)$$ ，常见的损失函数有：

* 平方损失： $$l(y_i,\hat{y}_i)=(y_i-\hat{y}_i)^2$$ 
* Logistic损失： $$l(y_i,\hat{y}_i)=y_i\ln (1+e^{-\hat{y}_i})+(1-y_i)\ln(1+e^{\hat{y}_i})$$ 

2、 $$\Omega(\theta)$$ ：正则化项，之所以要引入它是因为我们的目标是希望生成的模型能准确地预测新的样本（即应用于测试数据集），而不是简单地拟合训练集的结果（这样会导致过拟合）。所以需要在保证模型“简单”的基础上最小化训练误差，这样得到的参数才具有好的泛化性能。而正则化项就是用于惩罚复杂模型，避免模型过分拟合训练数据。常用的正则有 $$L^1$$ 正则与 $$L^2$$ 正则：

* $$L^1$$ 正则\(Lasso\)： $$\Omega(w)=\lambda||w||_1$$ 
* $$L^2$$ 正则： $$\Omega(w)=\lambda||w||^2$$ 

上述目标函数的设计来自于统计学习里面的一个重要概念叫做Bias-variance tradeoff（偏差-方差权衡），比较感性的理解，Bias可以理解为假设我们有无限多数据的时候，可以训练出最好的模型所拿到的误差。而Variance是因为我们只有有限数据，其中随机性带来的误差。目标中误差函数鼓励我们的模型尽量去拟合训练数据，这样相对来说最后的模型会有比较少的Bias。而正则化项则鼓励更加简单的模型。因为当模型简单之后，有限数据拟合出来结果的随机性比较小，不容易过拟合，使得最后模型的预测更加稳定。

## XGBoost原理

### 目标函数及二阶泰勒展开

在XGBoost中，选择树模型为基学习器，我们需要正则的对象，或者说需要控制复杂度的对象就是这 $$K$$ 颗树,通常树的参数有树的深度，叶子节点的个数，叶子节点值的取值（XGBoost里称为权重weight\)。所以，我们的目标函数形式如下：

                                                        $$\text{Obj}(\theta)=\sum\limits_{i=1}^nL(y_i,\hat{y}_i)+\sum\limits_{i=k}^K\Omega(f_k)$$ 

其中第一部分是训练损失，如上面所述的平方损失或者Logistic Loss等，第二部分是每棵树的复杂度的和。因为现在我们的参数可以认为是在一个函数空间里面，我们不能采用传统的如SGD之类的算法来学习我们的模型，因此我们会采用一种叫做additive training的方式。即每次迭代生成一棵新的回归树，从而使预测值不断逼近真实值（即进一步最小化目标函数）。每一次保留原来的模型不变，加入一个新的函数 $$f$$ 到模型里面：

                                                                           $$\hat{y}_i^0=c$$ （ $$c$$ 为常数）

                                                                      $$\hat{y}_i^1=\hat{y}_i^0+f_1(x_i)$$ 

                                                                      $$\hat{y}_i^2=\hat{y}_i^1+f_2(x_i)$$ 

                                                                   $$\hat{y}_i^K=\hat{y}_i^{K-1}+f_K(x_i)$$ 

所以，对于第 $$K$$ 次的目标函数为：

                                                           $$\text{Obj}^K=\sum\limits_iL(y_i,\hat{y}_i^K)+\Omega(f_k)+c$$ 

其中 $$c$$ 为前 $$K-1$$ 棵树的正则化项和，是一个常数。

根据 $$\hat{y}_i^K=\hat{y}_i^{K-1}+f_K(x_i)$$ ，进一步为：

                                                 $$\text{Obj}^K=\sum\limits_iL(y_i,\hat{y}_i^{K-1}+f_K(x_i))+\Omega(f_K)+c$$ 

上面的式子意思很明显，只需要寻找一棵合适的树 $$f_K$$ 使得目标函数最小。然后不断地迭代 $$K$$ 次就可以完成 $$K$$ 个学习器的训练。那么我们这棵树到底怎么找呢？在GBDT里面（当然GBDT没有正则），我们的树是通过拟合上一棵树的负梯度值，建树的时候采用的启发式准则，如MSE等。然而，在XGBoost里面，它是通过对损失函数进行二阶泰勒展开：

                                                 $$f(x+\Delta x)=f(x)+f'(x)\Delta x+\frac{1}{2}f''(x)\Delta x^2$$ 

对损失函数二阶泰勒展开：

  $$\sum\limits_iL(y_i,\hat{y}_i^{K-1}+f_K(x_i))=\sum\limits_i[L(y_i,\hat{y}_i^{K-1})+L'(y_i,\hat{y}_i^{K-1})f_K(x_i)+\frac{1}{2}L''(y_i,\hat{y}_i^{K-1})f^2_K(x_i)]$$

注意的是，这里的 $$y_i$$ 是标签值，是个常数，而 $$\hat{y}_i^{K-1}$$ 是第 $$K-1$$ 次学习的结果，也是个常数。所以只要把变化量 $$\Delta x$$ 看成我们需要学习的模型 $$f_K(x)$$ ，把 $$\hat{y}_i^{K-1}$$ 看作 $$x$$ 就可以展成上面的样子。

这里，我们用 $$g_i$$ 记为第 $$i$$ 个样本损失函数的一阶导， $$h_i$$ 记为第 $$i$$ 个样本损失函数的二阶导。

                                                                   $$g_i = L'(y_i,\hat{y}_i^{K-1})$$ 

                                                                   $$h_i = L''(y_i,\hat{y}_i^{K-1})$$ 

上面两式非常重要，贯串整个树的创建（分裂，叶子节点值的计算）。同时二阶导式子是我们利用XGBoost做特征选择时的一个评价指标。 所以我们可以得到我们进化后的目标函数：

                                $$\sum\limits_i[L(y_i,\hat{y}_i^{K-1})+g_if_K(x_i)+\frac{1}{2}h_if_K^2(x_i)]+\Omega(f_K)+c$$ 

 这里，我们先回忆一下，一颗树用数学模型来描述是什么样，很简单其实就是一个分段函数。比如有下面一颗树。

![](../../../../.gitbook/assets/20180309233049588.png)

                                                         $$f(x)=\begin{cases}0.444444\ \ \ x_1<10\\ -0.4\ \ \ \ \ \ \ \ \ \  x_1>=10\end{cases}$$ 

 也就是说，一棵树其实可以由一片区域以及若干个叶子节点来表达。 而同时，构建一颗树也是为了找到每个节点的区域以及叶子节点的值。

 也就说可以有如下映射的关系 $$f_K(x)=w_{q(x)}$$ 。其中 $$q(x)$$ 叶子节点的编号（从左往右遍）。 $$w$$ 是叶子节点的取值。即对于任意一个样本 $$x$$ ，其最后会落在树的某个叶子节点上，其值为 $$w_{q(x)}$$ 。

既然一棵树可以用叶子节点来表达，我们可以对叶子节点进行惩罚（正则），比如取 $$L^2$$ 正则，以及我们控制一下叶子节点的个数 $$T$$ ，那么正则项有：

                                                           $$\Omega(f_K)=\frac{1}{2}\lambda\sum\limits_i^T||w_j||^2+\gamma T$$ 

正则为什么可以控制模型复杂度呢？有很多角度可以看这个问题，最直观就是，我们为了使得目标函数最小，自然正则项也要小，正则项要小，叶子节点个数 $$T$$ 要小（叶子节点个数少，树就简单）。

而为什么要对叶子节点的值进行 $$L^2$$ 正则，这个可以参考一下逻辑回归里面进行正则的原因，简单的说就是逻辑回归没有加正则，整个 $$w$$ 的参数空间是无限大的，只有加了正则之后，才会把 $$w$$ 的解规范在一个范围内。（对此困惑的话可以跑一个不带正则的逻辑回归，每次出来的权重 $$w$$ 都不一样，但是Loss都是一样的，加了 $$L^2$$ 正则后，每次得到的 $$w$$ 都是一样的）。

这个时候，我们的目标函数（移除常数项后，注意： $$L(y_i,\hat{y}_i^{K-1})$$ 也是常数）就可以改写成这样（用叶子节点表达）：

                                               $$\sum\limits_i[g_iw_q(x_i)+\frac{1}{2}h_iw^2_{q(x_i)}]+\frac{1}{2}\lambda \sum\limits_j^T||w||^2+\gamma T$$ 

 其实我们可以进一步化简，也是最重要的一步化简（上式展开之后按照叶子节点编号进行合并后可以得到下式）。那么最后可以化简成：

                                                    $$\sum\limits_{j=1}^T[(\sum\limits_{(i\in I_j)}g_i)w_j+\frac{1}{2}(\sum\limits_{(i\in I_j)}h_i+\lambda)w^2_j]+\gamma T$$ 

下面，我们把 $$\sum\limits_{(i\in I_j)}g_i$$ 记为 $$G_i$$ ，把 $$\sum\limits_{(i\in I_j)}h_i$$ 记为 $$H_i$$ ，那么目标函数可以进一步简化为：

                                                             $$\sum\limits_{j=1}^T[G_iw_j+\frac{1}{2}(H_i+\lambda)w^2_j]+\gamma T$$ 

我们做了这么多，其实一直都是在对二阶泰勒展开后的式子化简，其实刚展开的时候就已经是一个二次函数了，只不过化简到这里能够把问题看的更加清楚。所以对于这个目标函数，我们对 $$w_j$$ 求导， 然后代入极值点，可以得到一个极值：

                                                                         $$w^*=-\frac{G_i}{H_i+\lambda}$$ 

                                                              $$\text{Obj}=-\frac{1}{2}\sum\limits_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T$$ 

上面第一个式子即为叶子节点取值的表达式，第二个式子即为目标函数的值。

值得注意的是：在GBDT中，不同的损失函数有不同的叶子节点的取值，而在Xgboost里，叶子节点取值的表达式很简洁，推导起来也比GBDT的要简便许多。

### 分裂准则

到这里，我们一直都是在围绕目标函数进行分析，这个到底是为什么呢？这个主要是为了后面我们寻找 $$f_k(x)$$ ，也就是建树的过程。

具体来说，我们回忆一下建树的时候需要做什么，建树的时候最关键的一步就是选择一个分裂的准则，也就如何评价分裂的质量。比如在前面文章GBDT的介绍里，我们可以选择MSE，MAE来评价我们的分裂的质量，但是，我们所选择的分裂准则似乎不总是和我们的损失函数有关，因为这种选择是启发式的。 比如，在分类任务里面，损失函数可以选择logloss，分裂准确选择MSE，这样看来，似乎分裂的好坏和我们的损失并没有直接挂钩。

但是，在Xgboost里面，我们的分裂准则是直接与损失函数挂钩的准则，这个也是Xgboost和GBDT一个很不一样的地方。具体来说，XGBoost选择 $$\text{Gain}=\text{Obj}_C-\text{Obj}_L-\text{Obj}_R$$ ：

                                            $$\text{Gain}=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}]-\gamma$$ 

其实选择这个作为准则的原因很简单也很直观。我们这样考虑，由 $$\text{Obj}=-\frac{1}{2}\sum\limits_{j=1}^T\frac{G_j^2}{H_j+\lambda}+\gamma T$$ 知道，对于一个节点，假设我们不分裂的话，此时的损失为 $$-\frac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}$$ 。假设在这个节点分裂的话，分裂之后左右叶子节点的损失分别为： $$-\frac{G_L^2}{H_L^2+\lambda}$$ 、 $$-\frac{G_R^2}{H_R^2+\lambda}$$ 。

既然要分裂的时候， 我们当然是选择分裂成左右子节点后，损失减少的最多。也就是找到一种分裂有：

                                                      $$\max[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}]$$ 

那么 $$\gamma$$ 的作用是什么呢？利用 $$\gamma$$ 可以控制树的复杂度，进一步来说，利用 $$\gamma$$ 来作为阈值，只有大于 $$\gamma$$ 时候才选择分裂。这个其实起到了预剪枝的作用。 最后就是如何得到左右子节点的样本集合？这个和普通的树一样，都是通过遍历特征所有取值，逐个尝试。

 至此，我们把Xgboost的基本原理阐述了一遍。我们总结一下，其实就是做了以下几件事情：

* 1、在损失函数的基础上加入了正则项
* 2、对目标函数进行二阶泰勒展开
* 3、利用推导得到的表达式作为分裂标准，来构建一棵树

### 算法流程

XGBoost核心部分的算法流程图如下：

![](../../../../.gitbook/assets/20180310232620896.png)

## 手动计算还原XGBoost过程

上面阐述了整个流程，有一些细节的地方可能都说的不太清楚。在这里，我以一个简单的UCI数据集，一步一步地和大家演算整个XGBoost的过程。

### 数据集、参数设置以及损失函数

 数据集的样本条数只有15条，2个特征。具体如下：

![](../../../../.gitbook/assets/20181008161505405.png)

这里为了简单起见，树的深度设置为3\(max\_depth=3\)，树的颗数设置为2\(num\_boost\_round=2\),学习率为0.1\(eta=0.1\)。另外再设置两个正则的参数， $$\lambda=1,\ \gamma=0$$ 。损失选择logloss。 由于后面需要用到logloss的一阶导数以及二阶导数，这里先简单推导一下：

                                     $$L(y_i,\hat{y}_i)=y_i\ln(1+e^{-\hat{y}_i})+(1-y_i)\ln(1+e^{\hat{y}_i})$$ 

两边对 $$\hat{y}_i$$ 求一阶导数：

                                                $$L'(y_i,\hat{y}_i)=y_i\frac{-e^{-\hat{y}_i}}{1+e^{-\hat{y}_i}}+(1-y_i)\frac{e^{\hat{y}_i}}{1+e^{\hat{y}_i}}$$ 

                                          $$\Rightarrow L'(y_i,\hat{y}_i)=y_i\frac{-1}{1+e^{-\hat{y}_i}}+(1-y_i)\frac{1}{1+e^{\hat{y}_i}}$$ 

                                          $$\Rightarrow L'(y_i,\hat{y}_i)=y_i*(y_{i,pred}-1)+(1-y_i)*y_{i,pred}$$ 

                                          $$\Rightarrow L'(y_i,\hat{y}_i)=y_{i,pred}-y_i$$ 

其中， $$y_{i,pred}=\frac{1}{1+e^{-\hat{y}_i}}$$ 

在一阶导的基础上再求一次导（其实就是sigmoid函数求导）：

                                                    $$L''(y_i,\hat{y}_i)=y_{i,pred}*(1-y_{i,pred})$$ 

所以样本的一阶导数值

                                                    $$g_i = y_{i,pred}-y_i$$ 

样本的二阶导数值

                                                    $$h_i = y_{i,pred}*(1-y_{i,pred})$$ 

### 建立第一棵树（k=1）

 建树的时候从根节点开始\(Top-down greedy\)，在根节点上的样本有ID1～ID15。那么回顾Xgboost的算法流程，我们需要在根节点处进行分裂，分裂的时候需要计算式子：

                                            $$\text{Gain}=\frac{1}{2}[\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{(G_L+G_R)^2}{(H_L+H_R)+\lambda}]-\gamma$$ 

式子表达的是：在节点处把样本分成左子节点和右子节点两集合。分别求两集合的 $$G_L,H_L,G_R,H_R$$ ，然后计算增益 $$\text{Gain}$$ 。

而这里，其实可以先计算每个样本的一阶导数值和二阶导数值，但是这里你可能碰到了一个问题，那就是第一颗树的时候每个样本的预测的概率是多少？这里和GBDT一样，应该说和所有的Boosting算法一样，都需要一个初始值。而在XGBoost里面，对于分类任务只需要初始化为 $$(0,1)$$ 中的任意一个数都可以。具体来说就是参数base\_score。（其默认值是 $$0.5$$ 。值得注意的是base\_score是一个经过sigmod映射的值，可以理解为一个概率值，提这个原因是后面建第二颗树会用到，需要注意这个细节）

 这里我们也设base\_score= $$0.5$$ 。然后我们可以计算每个样本的一阶导数值和二阶导数值了。具体如下表

![](../../../../.gitbook/assets/20180310183000696.png)

比如说对于ID=1样本， $$g_1=y_{1,pred}-y_1=0.5-0=0.5$$ ， $$h_1=y_{1,pred}*(1-y_{1,pred})=0.5*(1-0.5)=0.25$$ 。那么把样本如何分成两个集合呢？这里就是上面说到选取一个最佳的特征以及分裂点使得 $$\text{Gain}$$ 最大。

比如说对于特征 $$x_1$$ ，一共有 $$\{1,2,3,6,7,8,9,10\}$$ 这 $$8$$ 种取值。可以得到以下这么多划分方式。

以 $$1$$ 为划分时（ $$x_1<1$$ ）：

 左子节点包含的样本 $$I_{L}=\{\}$$ 

 右子节点包含的样本 $$I_R=\{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15\}$$ 

那么左子节点为空， $$G_L=0$$ 和 $$H_L=0$$ 

## Source

{% embed url="https://blog.csdn.net/anshuai\_aw1/article/details/82970489\#\_604" %}

{% embed url="https://blog.csdn.net/anshuai\_aw1/article/details/85093106" %}



