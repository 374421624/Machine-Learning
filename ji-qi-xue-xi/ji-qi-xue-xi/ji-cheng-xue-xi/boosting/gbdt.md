# GBDT

提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树。提升树模型可以表示为决策树的加法模型：

                                                               $$f_m(x)=\sum\limits_{m=1}^MT(x;\Theta_m)$$ 

其中， $$T(x;\Theta_m)$$ 表示决策树； $$\Theta_m$$ 为决策树的参数； $$M$$ 为树的个数。

## 提升树算法

提升树算法采用前向分步算法。首先确定初始提升树 $$f_0(x)=0$$ ，第 $$m$$ 步的模型是

                                                            $$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$$ 

其中， $$f_{m-1}(x)$$ 为当前模型，通过经验风险极小化确定下一棵决策树的参数 $$\Theta_m$$ ，

                                              $$\hat{\Theta}_m=\arg\min\limits_{\Theta_m}\sum\limits_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))$$ 

下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括平方误差损失函数的回归问题，用指数函数的分类问题，以及用一般损失函数的一般决策问题。

### 二分类问题

对于二分类问题，提升树算法只需将AdaBoost中的基本分类器限制为二分类树即可，可以说这时的提升树算法是AdaBoost算法的特殊情况。

### 回归问题

已知一个训练数据集 $$T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},\ x_i\in \mathcal{X}\subseteq R^n$$ ， $$\mathcal{X}$$ 为输入空间， $$y_i\in\mathcal{Y}\subseteq R$$ ， $$\mathcal{Y}$$ 为输出空间。如果将输入空间 $$\mathcal{X}$$ 划分为 $$J$$ 个互不相交的区域 $$R_1,R_2,\dots,R_J$$ ，并且在每个区域上确定输出的常量 $$c_j$$ ，那么树可表示为

                                                                $$T(x;\Theta)=\sum\limits_{j=1}^Jc_jI(x\in R_j)$$ 

其中，参数 $$\Theta=\{(R_1,c_1),(R_2,c_2),\dots,(R_J,c_J)\}$$ 表示树的区域划分和各区域上的常数。 $$J$$ 是回归树的复杂度即叶结点个数。

回归问题提升树使用以下前向分布算法：

* $$f_0(x)=0$$ 
* $$f_m(x)=f_{m-1}(x)+T(x;\Theta_m),\ \ m=1,2,\dots,M$$ 
* $$f_M(x)=\sum\limits_{m=1}^MT(x;\Theta_m)$$ 

在前向分步算法的第 $$m$$ 步，给定当前模型 $$f_{m-1}(x)$$ ，需求解

                                                  $$\hat{\Theta}_m=\arg\min\limits_{\Theta_m}\sum\limits_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))$$ 

得到 $$\hat{\Theta}_m$$ ，即第 $$m$$ 颗树的参数。当采用平方误差损失函数时，

                                                                    $$L(y,f(x))=(y-f(x))^2$$ 

其损失变为

    

## 梯度提升

