# GBDT

提升方法实际采用加法模型（即基函数的线性组合）与前向分布算法。以决策树为基函数的提升方法称为提升树。提升树模型可以表示为决策树的加法模型：

                                                               $$f_m(x)=\sum\limits_{m=1}^MT(x;\Theta_m)$$ 

其中， $$T(x;\Theta_m)$$ 表示决策树； $$\Theta_m$$ 为决策树的参数； $$M$$ 为树的个数。

## 提升树算法

提升树算法采用前向分步算法。首先确定初始提升树 $$f_0(x)=0$$ ，第 $$m$$ 步的模型是

                                                            $$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$$ 

其中， $$f_{m-1}(x)$$ 为当前模型，通过经验风险极小化确定下一棵决策树的参数 $$\Theta_m$$ ，

                                              $$\hat{\Theta}_m=\arg\min\limits_{\Theta_m}\sum\limits_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))$$ 

下面讨论针对不同问题的提升树学习算法，其主要区别在于使用的损失函数不同。包括平方误差损失函数的回归问题，用指数函数的分类问题，以及用一般损失函数的一般决策问题。

### 二分类问题

对于二分类问题，提升树算法只需将AdaBoost中的基本分类器限制为二分类树即可，可以说这时的提升树算法是AdaBoost算法的特殊情况。

### 回归问题

已知一个训练数据集 $$T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},\ x_i\in \mathcal{X}\subseteq R^n$$ ， $$\mathcal{X}$$ 为输入空间， $$y_i\in\mathcal{Y}\subseteq R$$ ， $$\mathcal{Y}$$ 为输出空间。如果将输入空间 $$\mathcal{X}$$ 划分为 $$J$$ 个互不相交的区域 $$R_1,R_2,\dots,R_J$$ ，并且在每个区域上确定输出的常量 $$c_j$$ ，那么树可表示为

                                                                $$T(x;\Theta)=\sum\limits_{j=1}^Jc_jI(x\in R_j)$$ 

其中，参数 $$\Theta=\{(R_1,c_1),(R_2,c_2),\dots,(R_J,c_J)\}$$ 表示树的区域划分和各区域上的常数。 $$J$$ 是回归树的复杂度即叶结点个数。

回归问题提升树使用以下前向分布算法：

* $$f_0(x)=0$$ 
* $$f_m(x)=f_{m-1}(x)+T(x;\Theta_m),\ \ m=1,2,\dots,M$$ 
* $$f_M(x)=\sum\limits_{m=1}^MT(x;\Theta_m)$$ 

在前向分步算法的第 $$m$$ 步，给定当前模型 $$f_{m-1}(x)$$ ，需求解

                                            $$\hat{\Theta}_m=\arg\min\limits_{\Theta_m}\sum\limits_{i=1}^NL(y_i,f_{m-1}(x_i)+T(x_i;\Theta_m))$$ 

得到 $$\hat{\Theta}_m$$ ，即第 $$m$$ 颗树的参数。当采用平方误差损失函数时，

                                                              $$L(y,f(x))=(y-f(x))^2$$ 

其损失变为

               $$L(y,f_{m-1}(x)+T(x;\Theta_m))=[y-f_{m-1}(x)-T(x;\Theta_m)]^2=[r-T(x;\Theta_m)]$$ 

这里， $$ r=y-f_{m-1}(x)$$ 即当前模型拟合数据的残差。所以，对回归问题的提升树算法来说，只需简单地拟合当前模型的残差。

#### 回归问题的提升树算法

输入：训练数据集 $$T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in\mathcal{X}\subseteq R^n,y_i\in\mathcal{Y}\subseteq R$$ 

输出：提升树 $$f_M(x)$$ 

（1）初始化 $$f_0(x)=0$$ 

（2）对 $$m = 1,2,\dots,M$$ 

* （a）按 $$ r_{mi}=y_i-f_{m-1}(x_i)$$计算残差
* （b）拟合残差 $$r_{mi}$$ 学习一个回归树，得到 $$T(x;\Theta_m)$$ 
* （c）更新 $$f_m(x)=f_{m-1}(x)+T(x;\Theta_m)$$ 

（3）得到回归提升树： $$f_M(x)=\sum\limits_{m=1}^MT(x;\Theta_m)$$ 

#### 举例如下

| $$x_i$$  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $$y_i$$  | 5.56 | 5.70 | 5.91 | 6.40 | 6.80 | 7.05 | 8.90 | 8.70 | 9.00 | 9.05 |

对于 $$m = 1$$ ：

求 $$f_1(x)$$ 即回归树 $$T_1(x)$$ ，首先通过以下优化问题：

                                        $$\min\limits_s[\min\limits_{c_1}\sum\limits_{x_i\in R_1}(y_i-c_1)^2+\min\limits_{c_2}\sum\limits_{x_i\in R_2}(y_i-c_2)^2]$$ 

求解训练数据的切分点 $$s$$ ：

                                                 $$R_1=\{x|x\leq s\}$$           $$R_2=\{x|x> s\}$$ 

容易求得在 $$R_1,R_2$$ 内部使平方损失误差达到最小值的 $$c_1,c_2$$ 为

                                                $$c_1=\frac{1}{N_1}\sum\limits_{x_i\in R_1}y_i$$               $$c_2 = \frac{1}{N_2}\sum\limits_{x_i\in R_2}y_i$$ 

这里 $$N_1,N_2$$ 是 $$R_1,R_2$$ 的样本点数

求训练数据的切分点。根据所给数据，考虑如下切分点：1.5, 2.5, 3.5, 4.5, 6.5, 7.5, 8.5, 9.5

对各切分点，求出相应的 $$R_1,R_2,c_1,c_2$$ 及 $$m(s)=\min\limits_{c_1}\sum\limits_{x_i\in R_1}(y_i-c_1)^2+\min\limits_{c_2}\sum\limits_{x_i\in R_2}(y_i-c_2)^2$$ 

例如，当 $$s= 1.5$$ 时， $$R_1=\{1\}$$ ， $$R_2=\{2,3,\dots,10\}$$ ， $$c_1=5.56$$ ， $$c_2=7.50$$ ，

 $$m(s)=\min\limits_{c_1}\sum\limits_{x_i\in R_1}(y_i-c_1)^2+\min\limits_{c_2}\sum\limits_{x_i\in R_2}(y_i-c_2)^2=0+15.72=15.72$$ 

现将 $$s$$ 及 $$m(s)$$ 的计算结果列表如下

| $$s$$  | 1.5 | 2.5 | 3.5 | 4.5 | 5.5 | 6.5 | 7.5 | 8.5 | 9.5 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $$m(s)$$  | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |

由上表可知，当 $$s = 6.5$$ 时 $$m(s)$$ 达到最小值，此时 $$R_1=\{1,2,\dots,6\}$$ ， $$R_2=\{7,8,9,10\}$$ ， $$c_1 = 6.24$$ ， $$c_2 = 8.91$$ ，所以回归树 $$T_1(x)$$ 为

                                                      $$T_1(x)=\begin{cases}6.24,\ \ x<6.5\\ 8.91,\ \ x\geq6.5\end{cases}$$ 

                                                      $$f_1(x) = T_1(x)$$ 

用 $$f_1(x)$$ 拟合训练数据的残差 $$r_{2i}=y_i-f_1(x),\ \ i=1,2,\dots,10$$ 

| $$x_i$$  | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 10 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| $$r_{2i}$$  | -0.68 | -0.54 | -0.33 | 0.16 | 0.56 | 0.81 | -0.01 | -0.21 | 0.09 | 0.14 |

用 $$f_1(x)$$ 拟合训练数据的平均损失误差： $$L(y,f_1(x))=\sum\limits_{i=1}^{10}(y_i-f_1(x_i))^2=1.93$$ 

对于 $$m = 2$$ ：

求 $$T_2(x)$$ ，方法与求 $$T_1(x)$$ 一样，只是拟合的数据为上一步的残差。可以得到

                                                      $$T_1(x)=\begin{cases}-0.52,\ \ x<3.5\\ 0.22,\ \ \ \ \ x\geq3.5\end{cases}$$ 

                       $$f_2(x) = f_1(x)+T_2(x)=\begin{cases}-0.52+6.24=5.72,\ \ x<3.5\\ 6.24+0.22=6.46,\ \ \ \ \  3.5\leq x<6.5\\ 8.91+0.22=9.13,\ \ \ \ \ \ x\geq6.5\end{cases}$$ 

用 $$f_2(x)$$ 拟合训练数据的平方损失误差是： $$L(y,f_2(x))=\sum\limits_{i=1}^{10}(y_i-f_2(x_i))^2=0.79$$ 

对于 $$m = 3,4,\dots,M $$：

继续按上述方法计算，直至求得拟合训练数据的平方误差到可接受范围。

## 梯度提升

提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。但对一般损失函数而言，往往每一步优化并不那么容易。针对这一问题，梯度提升算法利用最速下降法的近似方法，其核心是利用损失函数的[负梯度](https://zhuanlan.zhihu.com/p/33260455)在当前模型的值

                                                                $$-[\frac{\partial L(y,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1}(x)}$$ 

作为回归问题提升树算法中的残差的近似值，拟合一个回归树。

#### 梯度提升算法

输入：训练数据集 $$T = \{(x_1,y_1),(x_2,y_2),\dots,(x_N,y_N)\},x_i\in \mathcal{X}\subseteq R^n,y_i\in \mathcal{Y}\subseteq R$$

            损失函数 $$L(y,f(x))$$ 

输出：回归树 $$\hat{f}(x)$$ 

（1）初始化： $$f_0(x)=\arg\min\limits_c\sum\limits_{i=1}^NL(y_i,c)$$ 

（2）对 $$m = 1,2,\dots,M$$ 

* （a）对 $$i=1,2,\dots,N$$ ，计算
*           $$r_{mi} = -[\frac{\partial L(y_i,f(x_i))}{\partial f(x_i)}]_{f(x)=f_{m-1(x)}}$$ 
* （b）对 $$r_{mi}$$ 拟合一个回归树，得到第 $$m$$ 棵树的叶结点区域 $$R_{mj},j=1,2,\dots,J$$ 
* （c）对 $$j = 1,2,\dots,J$$ ，计算
*           $$c_{mj}=\arg\min\limits_c\sum\limits_{x_i\in R_{mi}}L(y_i,f_{m-1}(x_i)+c)$$ 
* （d）更新 $$f_m(x)=f_{m-1}(x)+\sum\limits_{j=1}^Jc_{mi}I(x\in R_{mi})$$ 

（3）得到回归树： $$\hat{f}(x)=f_M(x)=\sum\limits_{m=1}^M\sum\limits_{j=1}^Jc_{mi}I(x\in R_{mi})$$ 

算法第（1）步初始化，估计使损失函数极小化的常数值，它是只有一个根节点的数。

第（2a）步计算损失函数的负梯度在当前模型的值，将它作为残差的估计。对于平方损失函数，它就是通常所说的残差；对于一般损失函数，它就是残差的近似值。

第（2b）步估计回归树叶结点区域，以拟合残差的近似值。

第（2c）步利用线性搜索估计叶结点区域的值，是损失函数极小化。

第（2d）步更新回归树

第（3）步得到输出的最终模型。

