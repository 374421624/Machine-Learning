# 模型评估与选择

## 经验误差与过拟合

$$m$$ 个样本中有 $$a $$ 个分类错误，则

                        错误率： $$E = a/m$$              精度： $$Acc = 1 - E = 1 - p/m$$ 

实际预测输出与样本真实输出的差异为“”误差”，在训练集上的误差为“训练误差”\(training error\)或“经验误差”，在新样本上的误差为“泛化误差”。

我们事先并不知道新样本什么样，所以实际能做的是努力使经验误差最小化。然而，我们真正希望的是，在新样本能表现很好的学习器。当一个学习器把训练样本学的“太好了”，很可能把训练样本的特点当成了一般性质，导致泛化能力下降，这种情况成为“过拟合”，相对的“欠拟合”即对训练样本未学好。

## 评估方法

通常我们通过实验测试来对学习器的泛化误差进行评估而做出选择，为此，需要使用一个“测试集”来测试学习器对新样本的判别能力。下面介绍几种常见的数据集 $$D$$ 划分训练集 $$S$$ 及测试集 $$T$$ 的做法。

另外，数据量充足时用留出法或交叉验证法，数据集比较小时可以使用自助法。

### 留出法

直接将数据集 $$D$$ 划分为两个互斥的集合，其中一个为训练集 $$S$$ ，另一个为测试集 $$T$$ ，即 $$D=S\cup T, \ \ S\cap T = \varnothing$$ 。需要注意的是训练/测试集的划分要尽可能保持数据分布的一致性，避免因数据划分过程引入额外的偏差而对最终结果产生影响，例如在分类任务中至少要保持样本的类别比例相似，如果从采样的角度来看，此类采样方式通常称为“分层采样”。

即便在给定训练/测试集的样本比例后，仍存在多种划分数据集 $$D$$ 的方式，且单次使用留出法得到的估计结果往往不够稳定可靠。在使用留出法时，一般采用若干次随机划分、重复进行实验评估后取平均值作为留出法的评估结果。此外，我们将 $$2/3$$ 至 $$4/5$$ 的样本用于训练，剩余样本用于测试，以保证训练集或测试集都不会太小导致结果差别大。

### 交叉验证法

先将数据集 $$D$$ 划分为 $$k$$ 个大小相同的互斥子集，即 $$D = D_1 \cup D_2   \dots \cup D_k,\ D_i\cap D_j = \varnothing \ (i \neq j)$$，每个子集 $$D_i$$ 都尽可能保持数据分布的一致性。然后每次用 $$k-1$$ 个子集的并集作为训练集，余下的哪个子集作为测试集，这样就得到了 $$k$$ 组训练/测试集，从而可进行 $$k$$ 次训练和测试，最终返回的是这 $$k$$ 个结果的均值。

与留出法相似，将数据集 $$D$$ 划分为 $$k$$ 个子集的同样存在多种划分方式。为减少因样本划分不同而引入的差别， $$k$$ 折交叉验证通常要随机使用不同的划分重复 $$p$$ 次。最终的评估结果是这 $$p$$ 次 $$k$$ 折交叉验证结果的均值，例如常见的有“10次10折交叉验证”。

另外留一法，即数据集 $$D$$ 中有 $$m$$ 个样本， $$k = m$$ 即为留一法，留一法训练集与实际数据集 $$D$$ 只差了1个样本，所以评估结果往往被认为较准确。而当数据集很大的时候，即需要计算 $$m$$ 个模型，消耗过大。

### 自助法

给定包含 $$m$$ 个样本的数据集 $$D$$ ，我们对它进行采样产生数据集 $$D'$$ ：每次随机从 $$D$$ 中挑选一个样本，并将其拷贝放入 $$D'$$ ，然后再将该样本放回初始数据集 $$D$$ 中，使得该样本在下次采集时仍有可能被采到；重复执行 $$m$$ 次后，我们就得到了包含 $$m$$ 个样本的数据集 $$D'$$ 。

显然， $$D$$ 中一部分样本会在 $$D'$$ 中出现多次，而另一部分样本不出现，样本在 $$m$$ 次采样中始终不被采到的概率是 $$(1-m)^m$$ ，取极限得到

                                                           $$\mathop{lim} \limits_{m\to \infty} (1- \frac{1}{m})^m \to \frac{1}{e} \approx 0.368$$ 

即初始数据集 $$D$$ 中约有36.8%的样本并未出现在采样数据集 $$D'$$ 中，于是我们可将 $$D$$ \$$D'$$ 用作训练集， $$D$$ 用作测试集。这样，实际评估的模型与期望评估的模型都使用 $$m$$ 个训练样本，而我们仍有数据总量约1/3的、没在训练集中出现的样本用于测试。

自主法在数据集小、难以划分训练/测试集时很有用，且能从初始数据集产生多个不同的训练集，对集成学习等方法有很大好处。但是自助法改变了原始数据分布，会引入估计偏差。

## 性能度量

性能度量：衡量模型泛化能力的评价标准

回归任务最常用的性能度量--“均方差\(Mean Squared Error\)”:

                  $$E(f;D) = \frac{1}{m} \sum \limits_{i=1}^m (f(x_i)-y_i)^2$$               $$E(f;D) = \int_{x~D}(f(x)-y)^2p(x)dx$$ 

### 错误率与精度

错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例。

对样例集 $$D$$，分类错误率定义为：

                                          $$E(f;D) = \frac{1}{m} \sum \limits_{i=1}^m \amalg (f(x_i) \neq y_i)$$ 

精度定义为：

                                    $$acc(f;D) = \frac{1}{m} \sum \limits_{i=1}^m \amalg (f(x_i) = y_i) = 1- E(f;D)$$ 

更一般的，对于数据分布 $$D$$ 和概率密度函数 $$p(\cdot)$$，错误率与精度可分别描述为：

$$E(f;D) = \int_{x~D} \amalg (f(x) \neq y)p(x)dx$$      $$acc(f;D) = \int_{x~D} \amalg (f(x) = y)p(x)dx = 1 -E(f;D)$$

### 查准率、查全率与F1

| 真实情况 | 预测结果 | 预测结果 |
| :---: | :---: | :---: |
|  | 正例 | 反例 |
| 正例 | TP（真正例） | FN（假反例） |
| 反例 | FP（假正例） | TN（真反例） |

#### 查准率\(Precision\) ： $$P = \frac{TP}{TP+FP}$$ 

#### 查全率\(Recall\)： $$R = \frac{TP}{TP+FN}$$ 

#### P-R曲线：

![](../../.gitbook/assets/20170113160327591.png)

#### F1度量： $$F1 = \frac{2\times P \times R}{P+R} = \frac{2 \times TP}{ALL + TP -TN}$$ 

#### F-score： $$F_\beta = \frac{1}{\alpha \times \frac{1}{P}+(1-\alpha)\times \frac{1}{R}} = \frac{(1+\beta^2)\times P \times R}{\beta^2\times P + R}$$ 

### [ROC与AUC](https://www.jianshu.com/p/c61ae11cc5f6)

很多学习器是为测试样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则分为正类，否则为反类。例如我们对每个样本出一个 $$[0.0,1.0]$$ 之间的实值，然后与 $$0.5$$ 比较，大于为正例，否则为反例。这个实值或概率预测结果的好坏直接决定了学习器的泛化能力。实际上，我们根据预测结果将测试样本排序，“最可能”是正例的排在最前面，“最不可能”是正例的排在最后面。这样，分类过程就相当于在这个排序中以某个“截断点”将样本分为两部分，前一部分判作正例，后部分为反例。

我们根据任务需求来采取不同的截断点，例如更重视“查准率”，则在排序靠前位置截断；若重视“查全率”，则选择较后位置。因此排序好坏直接影响学习器泛化能力好坏，ROC曲线既从这个角度出发来研究学习期的泛化能力。

ROC曲线的纵轴是“真正例率”\(True Positive Rate\)，横轴是“假正例率”\(False Positive Rate\)：

                                  $$TPR = \frac{TP}{TP+FN}$$                                     $$FPR = \frac{FP}{TN+FP}$$ 

AUC\(Area Under ROC Curve\)如名字所示，即ROC曲线下面积

![ROC&#x66F2;&#x7EBF;&#x548C;&#x5B83;&#x76F8;&#x5173;&#x7684;&#x6BD4;&#x7387;](../../.gitbook/assets/145616-ce8221a29d9c01ef.png)

理性情况下，我们有无限个样本，ROC是一条平滑的曲线，然而现实中，我们只有有限个数据，所以：

            ![](../../.gitbook/assets/145616-ca9d12fa24fe2842.png)  ![](../../.gitbook/assets/145616-2063bb79c3684a8a.png) 

### 代价敏感错误率与代价曲线

实践中有不同类型错误造成不同后果：比如体检，将健康人误诊断为患者，虽然增加进一步检查麻烦，但是要把患者误诊为健康人可能会丧失生命；再比如银行人脸识别门禁等等情况。为权衡不同类型错误所造成的不同损失，可为错误赋予“非均等代价”。

| 真实类别 | 预测类别 | 预测类别 |
| :---: | :---: | :---: |
| 真实类别 | 第0类 | 第1类 |
| 第0类 | 0 |  $$cost_{01}$$  |
| 第1类 | $$cost_{10}$$  | 0 |

之前指标我们只计算不同错误次数，代价敏感的错误率是对应错误乘以其错误代价：

$$E(f;D;cost)=\frac{1}{m}\left(\sum_{x_i \in D^+} I(f(x_i) \neq y_i)\times cost_{01} +\sum_{x_i\in D^-}I(f(x_i)\neq y_i)\times cost_{10}\right)$$ 

绘制代价曲线：代价曲线的横轴为正例概率代价 $$P(+)cost$$ ，纵轴为归一化代价 $$cost_{norm}$$ 

$$P(+)cost=\frac{p\times cost_{01}}{p\times cost{01} + (1-p)\times cost_{10}}$$          $$cost_{norm}=\frac{FNR\times p\times cost_{01} + FPR\times (1-p)\times cost_{10}}{p\times cost{01} + (1-p)\times cost_{10}}$$ 

 其中， $$p$$或 $$p(+)$$ 是样例为正例的概率；FNR是假反例率；FPR是假正例率。

![](../../.gitbook/assets/2017-09-02_165517.png)

## 比较检验

有了实验评估方法和性能度量，就可以对学习器性能进行比较了。但实际操作起来，我们并不能直接取得性能度量的值然后比大小，因为我们希望模型有很好的泛化能力，然而我们实验评估是获得测试集上的性能，两者的对比结果可能未必相同；测试集上的性能与测试集选取有很大关系，怎样论证测试集选取；机器学习算法很多有一定的随机性。

所以就需要用比较检验，基于检验结果我们可推断出有多大的把握学习器A的泛化性能比学习器B好。下面用错误率作为性能度量，用 $$\epsilon$$ 表示。

### 单个学习器泛化进行检验

泛化错误率为 $$\epsilon$$ 的学习器在一个样本上犯错的概率是 $$\epsilon$$ ；测试错误率 $$\hat{\epsilon}$$ 意味着在 $$m$$ 个测试样本中恰有 $$\hat{\epsilon}\times m$$ 个被误分类。测试误分类概率：

                                                    $$P(\hat{\epsilon};\epsilon)=\binom{m}{\hat{\epsilon}\times m}\epsilon^{\hat{\epsilon}\times m}(1-\epsilon)^{m-\hat{\epsilon}\times m}$$ 

给定测试错误率，则解 $$\partial P(\hat{\epsilon;\epsilon})/\partial \epsilon = 0$$ 可知， $$P(\hat{\epsilon};\epsilon)$$ 在 $$\epsilon=\hat{\epsilon}$$ 时最大， $$|\epsilon-\hat{\epsilon}|$$增大时$$P(\hat{\epsilon;\epsilon})$$减小

#### 二项检验

由上面所述， $$P(\hat{\epsilon;\epsilon})$$ 是符合二项分布的，我们可用二项检验来应对。考虑假设 $$\epsilon \leq \epsilon_0$$ ，则在 $$1-\alpha$$ （这里 $$1-\alpha$$ 反映了结论的“置信度”）的概率内所能观察到的最大错误率如下式计算：

                                              $$\overline{\epsilon} = max\ \epsilon \ \ \ s.t. \ \ \sum \limits_{i=\epsilon_0\times m+1}^m\binom{m}{i}\epsilon^i(1-\epsilon)^{m-i} <\alpha$$ 

此时若测试错误率 $$\hat{\epsilon}$$ 小于临界值 $$\overline{\epsilon}$$，则根据二项检验可得出结论：在 $$\alpha$$ 的显著度下，假设 $$\epsilon \leq \epsilon_0$$ 不能被拒绝，即能以 $$1-\alpha$$ 的置信度认为，学习器的泛化错误率不大于 $$\epsilon_0$$ ；否则该假设可被拒绝，即在 $$\alpha$$ 的显著度下可任务学习器的泛化错误率大于 $$\epsilon_0$$ 。

#### t检验

在很多时候我们并非仅做一次留出法估计，而是通过多次重复留出法或是交叉验证法等进行多次训练/测试，这样会得到多个测试错误率，此时可使用t检验\(t-test\)。

假定我们得到了 $$k$$ 个测试错误率， $$\hat{\epsilon_1},\hat{\epsilon_2},\dots,\hat{\epsilon_k}$$，则平均测试错误率 $$\mu$$ 和方差 $$\sigma^2$$ 为

                                                     $$\mu = \frac{1}{k}\sum \limits_{i=1}^{k}\hat{\epsilon_i}$$          $$\sigma^2 = \frac{1}{k-1}\sum \limits_{i=1}^{k}(\hat{\epsilon_i}-\mu)^2$$ 

考虑到这 $$k$$ 个测试错误率可看作泛化错误率 $$\epsilon_0$$ 的独立采样，则变量：

                                                                      $$\tau_t = \frac{\sqrt{k}(\mu-\epsilon_0)}{\sigma}$$ 

下图为服从自由度为 $$k-1$$ 的t分布，其中 $$k = 10$$ 

![](../../.gitbook/assets/v2-972478b7a55121dda9681c7174fc2cf1_hd.png)

对假设 $$\mu = \epsilon_0$$ 和显著度 $$\alpha$$ ，我们可计算出当测试错误率均为 $$\epsilon_0$$ 时，在 $$1-\alpha$$ 概率内能观测到的最大错误率，即临界值。这里考虑双边假设，如上图所示，两边阴影部分各有 $$\frac{\alpha}{2}$$ 的面积；假定阴影部分范围分别为 $$[-\infty,t_{-\frac{\alpha}{2}}]$$ 和 $$[t_{\frac{\alpha}{2}},\infty]$$ 。若平均错误率 $$\mu$$ 与 $$\epsilon_0$$ 之差 $$|\mu-\epsilon_0|$$位于临界值范围 $$[t_{-\frac{\alpha}{2}},t_{\frac{\alpha}{2}}]$$ 内，则不能拒绝假设 $$\mu = \epsilon_0$$ ，即可认为泛化错误率为 $$\epsilon_0$$ ，置信度为 $$1-\alpha$$ ；否则可拒绝该假设，即在该显著度下可认为泛化错误率与 $$\epsilon_0$$ 有显著不同。 $$\alpha$$ 常用取值的 $$0.05$$ 和 $$0.1$$ 。

### 不同学习器泛化进行检验

#### 交叉验证t检验

对两个学习器 $$A$$ 和 $$B$$ ，若我们使用 $$k$$ 折交叉验证法得到的测试错误率分别为 $$\epsilon^A_1,\epsilon^A_2,\dots,\epsilon^A_k$$ 和 $$\epsilon^B_1,\epsilon^B_2,\dots,\epsilon^B_k$$，其中 $$\epsilon_i^A$$ 和 $$\epsilon_i^B$$ 是在相同的第 $$i$$ 折训练/测试集上得到的结果，则可用 $$k$$ 折交叉验证“成对 $$t$$ 检验”来进行比较检验。这里的基本思想是若两个学习器的性能相同，则它们使用相同的训练/测试集得到的测试错误率应相同，即 $$\epsilon_i^A = \epsilon_i^B$$ 

具体来说，对 $$k$$ 折交叉验证产生的 $$k$$ 对测试错误率：先对没对结果求差， $$\Delta_i=\epsilon_i^A-\epsilon_i^B$$ ；若两个学习器性能相同，则差值均值应为零。因此，可根据差值 $$\Delta_1,\Delta_2,\dots,\Delta_k$$ 来对“学习器 $$A$$ 与 $$B$$ 性能相同”这个假设做 $$t$$ 检验，计算出差值的均值 $$\mu$$ 和方差 $$\sigma^2$$ ，在显著度 $$\alpha$$ 下，若变量

                                                                          $$\tau_t=|\frac{\sqrt{k}\mu}{\sigma}|$$ 

小于临界值 $$t_{\frac{\alpha}{2},k-1}$$ ，则假设不能被拒绝，即认为两个学习器的性能没有显著差别；否则可认为两个学习器的性能有显著差别，且平均错误率较小的那个学习器性能较优。这里 $$t_{\frac{\alpha}{2},k-1}$$ 是自由度为 $$k-1$$ 的 $$t$$ 分布上尾部累积分布为 $$\frac{\alpha}{2}$$ 的临界值。

欲进行有效的假设检验，一个重要前提是测试错误率均为泛化错误率的独立采样。然而，通常情况下由于样本有限，在使用交叉验证等实验估计方法时，不同轮次的训练集会有一定程度的重叠，这就使得测试错误率实际上并不独立，会导致过高估计假设成立的概率。为缓解这一问题，可采用“ $$5\times 2$$ 交叉验证”法。

$$5\times 2$$ 交叉验证是做 $$5$$ 次 $$2$$ 折交叉验证，在每次 $$2$$ 折交叉验证之前随机将数据打乱，使得 $$5$$ 次交叉验证中的数据划分不重复。对两个学习器 $$A$$ 和 $$B$$ ，第 $$i$$ 次 $$2$$ 折交叉验证将产生两对测试错误率，我们对它们分别求差，得到第 $$1$$ 折上的差值 $$\Delta_i^1$$ 和第 $$2$$ 折上的差值 $$\Delta_i^2$$ 。为缓解测试错误率的非独立性，我们仅计算第 $$1$$ 次 $$2$$ 折交叉验证的两个结果的平均值 $$\mu=0.5(\Delta_1^1+\Delta_1^2)$$ ，但对每次 $$2$$ 折实验的结果都计算出其方差 $$\sigma_i^2=(\Delta_i^1-\frac{\Delta_i^1+\Delta_i^2}{2})^2+(\Delta_i^2-\frac{\Delta_i^1+\Delta_i^2}{2})^2$$ 。变量

                                                                         $$\tau_t = \frac{\mu}{\sqrt{0.2\sum\limits_{i=1}^5\sigma^2_i}}$$ 

服从自由度为 $$5$$ 的 $$t$$ 分布，其双边检验的临界值 $$t_{\frac{\alpha}{2},5}$$ 。

#### McNemar检验

对二分类问题，使用留出法不仅可估计出学习器 $$A$$ 和 $$B$$ 的测试错误率，还可获得两学习器分类结果的差别，即两者都正确、都错误、一个正确一个错误的样本数，如下表所示

![](../../.gitbook/assets/20180325135156682140.png)

若我们做的假设是两学习器性能相同，则应有 $$e_{01}=e_{10}$$ ，那么变量 $$|e_{01}-e_{10}|$$ 应当服从正态分布。McNemar检验考虑变量

                                                                          $$\tau_{\chi^2}=\frac{(|e_{01}-e_{10}|-1)^2}{e_{01}+e_{10}}$$ 

服从自由度为 $$1$$ 的 $$\chi^2$$ 分布，即标准正态分布变量的平方。给定显著度 $$\alpha$$ ，当以上变量值小于临界值 $$\chi^2_{\alpha}$$ 时，不能拒绝假设，即认为两学习器的性能没有显著差别；否则拒绝假设，即认为两者性能有显著差别，且平均错误率较小的那个学习器性能较优。自由度为 $$1$$ 的 $$\chi^2$$ 检验的临界值当 $$\alpha=0.05$$ 时为 $$3.8415$$ ， $$\alpha=0.1$$ 时为 $$2.7055$$ 。

#### Friedman检验与Nemenyi后续检验

交叉验证 $$t$$ 检验和McNemar检验都是在一个数据集上比较两个算法的性能。而在很多时候，我们会在一组数据集上对多个算法进行比较。当有多个算法参与比较时，一种做法是在每个数据集上分别列出两两比较的结果，这时即可用交叉验证 $$t$$ 检验和McNemar检验；另一种方法更为直接，即使用基于算法排序的Friedman检验。

假定我们用 $$D_1,D_2,D_3,D_4$$ 四个数据集对算法 $$A,B,C$$ 进行比较。首先，使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果，然后再每个数据集上根据测试性能由好到坏排序，并将排名作为值赋予，若算法的测试性能相同，则评分序值。然后对四个数据集的结果计算平均序值

![](../../.gitbook/assets/ml14-46.png)

若“所有算法的性能相同”这个假设被拒绝，则说明算法的性能显著不同。这时需进行“后续检验”来进一步区分各算法。常用的有Nemenyi后续检验。Nemenyi检验计算出平均序值差别的临界值域

                                                                     $$CD=q_\alpha \sqrt{\frac{k(k+1)}{6N}}$$ 

## 偏差与方差

对学习算法除了通过实验估计其泛化性能，人们往往还希望了解它“为什么”具有这样的性能。“偏差-方差分解”是解释学习算法泛化性能的一种重要工具。

偏差-方差分解试图对学习算法的期望泛化错误率进行拆解。我们知道，算法在不同训练集上学得的结果很可能不同，即便这些训练集是来自同一个分布。对测试样本 $$x$$ ，令 $$y_D$$ 为 $$x$$ 在数据集中的标记， $$y$$ 为 $$x$$ 的真实标记， $$f(x;D)$$ 为训练集 $$D$$ 上学得模型 $$f$$ 在 $$x$$ 上的预测输出。以回归任务为例，学习算法的期望预测为：

                                                                      $$\overline{f}(x) = \mathbb{E}_D[f(x;D)]$$ 

使用样本数相同的不同训练集产生的方差为

                                                          $$var(x)=\mathbb{E}_D[(f(x;D)-\overline{f}(x))^2]$$ 

噪声为

                                                                      $$\varepsilon^2=\mathbb{E}_D[(y_D-y)^2]$$ 

期望输出与真实标记的差别称为偏差\(bias\)，即

                                                                    $$bias^2(x)=(\overline{f}(x)-y)^2$$ 

为便于讨论，假定噪声期望为零，即 $$\mathbb{E}_D[y_D-y]=0$$ 。通过简单的多项式展开合并，可对算法的期望泛化误差进行分解：

$$E(f;D)=\mathbb{E}_D[(f(x;D)-y_D)^2]$$ 

                   $$= \mathbb{E}_D[(f(x;D)-\overline{f}(x)+\overline{f}(x)-y_D)^2]$$ 

               $$= \mathbb{E}_D[(f(x;D)-\overline{f}(x))^2]+\mathbb{E}_D[(\overline{f}(x)-y_D)^2]+\mathbb{E}_D[2(f(x;D)-\overline{f}(x))(\overline{f}(x)-y_D)]$$

                    $$= \mathbb{E}_D[(f(x;D)-\overline{f}(x))^2]+\mathbb{E}_D[(\overline{f}(x)-y_D)^2]$$ 

                    $$= \mathbb{E}_D[(f(x;D)-\overline{f}(x))^2]+\mathbb{E}_D[(\overline{f}(x)-y+y-y_D)^2]$$  

   $$= \mathbb{E}_D[(f(x;D)-\overline{f}(x))^2]+\mathbb{E}_D[(\overline{f}(x)-y)^2]+\mathbb{E}_D[(y-y_D)^2]+2\mathbb{E}_D[(\overline{f}(x)-y)(y-y_D)]$$

                     $$= \mathbb{E}_D[(f(x;D)-\overline{f}(x))^2]+(\overline{f}(x)-y)^2+\mathbb{E}_D[(y_D-y)^2]$$ 

于是，

                                               $$E(f;D)=bias^2(x)+var(x)+\varepsilon^2$$ 

也就是说，泛化误差可分解为偏差、方差与噪声之和。

偏差度量了学习算法的期望预测与真是结果的偏离程度，即刻画了学习算法本身的拟合能力；方差度量了同样大小的训练集的变动所导致的学习性能的变化，即刻画了数据扰动所造成的影响；噪声则表达了在当前任务上任何学习算法所能达到的期望泛化误差的下界，即刻画了学习问题本身的难度。偏差-方差分解说明，泛化性能是由学习算法的能力、数据的充分性以及学习任务本身的难易程度所共同决定的。给定学习任务，为了取得更好的泛化能力，则需使偏差较小，即能够充分拟合数据，并且使方差较小，即使得数据扰动产生的影响小。

一般来说，偏差与方差是有冲突的，这成为偏差-f你观察窘境。下图给出了一个示意图。给定学习任务，假定我们能控制学习算法的训练程度，则在训练不足时，学习器的拟合能力不够强，训练数据的扰动不足以使学习器产生显著变化，此时偏差主导了泛化错误率；随着训练程度的加深，学习器的拟合能力逐渐增强，训练数据发生的扰动渐渐能被学习器学到，方差逐渐主导了泛化误差；在训练程度充足后，学习器的拟合能力已非常高，训练数据发生的轻微扰动都会导致学习器发生显著变化，若训练数据自身的，非全局的特性被学习器学到了，则将发生过拟合。

![](../../.gitbook/assets/download%20%281%29.jpeg)

## Source

{% embed url="https://github.com/chmx0929/UIUCclasses/blob/master/412DataMining/PDF/08ClassBasic.pdf" %}

{% embed url="https://www.jianshu.com/p/c61ae11cc5f6" %}



