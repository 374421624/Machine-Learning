# 点击预估

## CTR介绍

CTR（Click-Through-Rate）即点击通过率，是互联网广告常用的术语，指网络广告（图片广告/文字广告/关键词广告/排名广告/视频广告等）的点击到达率，即该广告的实际点击次数除以广告的展现量。点击率预估（Click-Through Rate Prediction）是互联网主流应用\(广告、推荐、搜索等\)的核心算法问题，包括Google、Facebook等业界巨头对这个问题一直进行着持续投入和研究。 CTR预估是互联网计算广告中的关键技术环节，预估准确性直接影响公司广告收入。广告领域的CTR预估问题，面临的是超高维离散特征空间中模式发现的挑战——如何拟合现有数据的规律，同时又具备推广性。

## CTR问题特点与挑战

以阿里定向广告为例介绍CTR预估问题的特点与挑战：

下图中可以看到手机淘宝端的定向广告形态。左边是首焦场景，在淘宝顶端的位置会有浮动的Banner广告。右边是往下滑动时候的导购场景（猜你喜欢区块），投放的是Item广告。这些不同形态的定向广告背后其实有一些内在的、从machine learning视角来看相似的特征。简单来说，可以归纳为几个方面，一个方面是广告中展现的创意图片，第二个是图片的文字信息，还有一些在背后看不到摸不到的统一的ID体系，比如某件商品是什么商品，属于哪个品牌等等信息。定向广告复杂多样的富媒介形态以及高维海量数据空间，给广告点击率预估问题带来了不小的挑战。

![](../../../../.gitbook/assets/v2-7067e1ff9e9d9ff6181e48b13115b8a0_hd.jpg)

下图是电商环境下CTR预估问题的数学化模拟。假设一位用户登录手机淘宝，我们首先可以拿到用户的一些历史行为数据，这些行为数据构成了我们对用户兴趣的表达刻画。那么下一步需要预估给用户展现某个候选商品candidate，用户发生点击/购买的概率是多少。那么如何实现预估？我们需要利用历史行为数据建模出用户的兴趣偏好。

将用户的行为按照时间排列，可以构成一个行为时间轴。每个时刻点可以称为行为结构体，它包含了一系列表征行为的关键信息：比如此刻的行为类型，点击or购买某个商品；某个商品的文本描述信息；对应的创意图片；行为发生的时间，行为发生的频次；或者行为背后的粒度体系是什么，对应的是什么商品、什么店铺以及什么品牌等等。

这些大量的行为信息可以足够表达用户的兴趣偏好。时间轴左边是历史的静态信息，称为feature；右边就是待预测的用户的未来行为，如点击行为（点击概率）、购买行为（购买概率）等等。电商场景下的广告预估问题相比于大家熟知的静态预估模型有更大的挑战。

![](../../../../.gitbook/assets/v2-7b22ad65f42ac479be3b388fba2dc934_r.jpg)

第一个挑战，在淘宝端每天有数亿的用户会登陆，并产生大量的行为。同时我们有海量的商品候选集，在淘宝中有大概10亿到20亿的商品，当然聚焦到广告商品，可能会有所减少，但依然达到了千万的数量级。如此，广告预估问题就变成了数亿用户与千万商品配对的点击概率预估问题，规模极其庞大。

第二个挑战，每个用户行为特征背后，有大量的信号源，比如图像信号、文字信号、品牌偏好信号等等，这些信号如何去捕捉，如何进行统一建模？

第三个挑战，在电商场景下用户的行为非常丰富，反映出用户的兴趣多样多变，寻找与建模用户点击某个广告商品背后的规律是高度非线性的问题。

## 传统CTR算法及其不足

业界传统的CTR预估解法是广义线性模型LR\(logistic regression，逻辑斯特回归\)+人工特征工程。LR使用了Logit变换将函数值映射到0~1区间，映射后的函数值就是CTR的预估值。LR这种线性模型很容易并行化，处理上亿条训练样本不是问题。但这种解法的不足是，因为线性模型的学习能力有限，需要引入大量的领域知识来人工设计特征以及特征之间的交叉组合来间接补充算法的非线性学习能力，非常消耗人力和机器资源，迁移性不够友好。

另外，目前业界也有一些效果不错的非线性模型不断被提出来，并被工程实践且取得不错效果，但这些模型都或多或少存在一些不足。比如Kernel方法，因为复杂度太高而不易实现；比如Tree based方法，这个是由Facebook团队在2014年首先提出，有效地解决了LR模型的特征组合问题，但缺点就是仍然是对历史行为的记忆，缺乏推广性；还有FM（factorization machine）模型，能自动学习高阶属性的权值，不用通过人工的方式选取特征来做交叉，但FM模型只能拟合特定的非线性模式，如最常用的2阶FM只能拟合特征之间的线性关系和二次关系。深度神经网络非线性拟合能力足够强，但面对广告这样的大规模工业级稀疏数据，适合数据规律的、具备推广性的网络结构业界依然在探索中，尤其是要做到端到端规模化上线，这里面的技术挑战依然很大。

所以，如何设计算法从大规模数据中挖掘出具有推广性的非线性模式即急需探索问题。

在Deep Learning时代启动初期，有着巨大的挑战，具体包括：

* 从数百维统计特征到数十亿离散特征，训练程序要做重大升级，从数据并行模式要升级到模型并行方式，且非线性模型复杂度高，需要充分利用数据的结构化特点进行加速；
* 在这种互联网尺度\(百亿参数&样本\)的数据上，模型能不能学习到兼具拟合能力与泛化能力的范式？
* 超大规模数据上的非凸优化\(MLR加入正则后进一步变成非光滑\)学术界鲜有先例。它的收敛性是一个巨大的问号。

## 基于深度学习的CTR预估算法演化

传统的图像领域或NLP领域内，深度学习已经取得非常多的成果，在大多的问题上成为了state-of-the-art的方法。如图中显示，不同的领域有不同的适用结构。如图像领域内的CNN结构、语音的RNN结构。那么回到广告领域内，究竟什么结构是合适的？广告预估问题有很强的特点，它的特征极其的大规模和稀疏。典型的数量级从百万级，千万级到数亿级都有，而且大都是0或1这类没有直接意义的数据。这个问题一度成为广告预估问题引入深入学习的关键点。

![](../../../../.gitbook/assets/v2-3da9adf6ccd48638031e703797e9372d_r.jpg)

### DSSM

如果说MLR模型是阿里巴巴初次对于深度学习方面的探索，在深度学习真正引入到广告预估问题中之后，出现了更多演变的模型。Deep Structured Semantic Model（DSSM）模型是微软2013年提出的。虽然在最初DSSM模型不是用于广告预估，但是现在看来，它为广告预估提供了一个很好的思路。这里主要关注下图中红色框内的部分，原理是把query/doc中的关键信息（Term Vector）提取出来进行简单的Word Hashing之后，把query/doc域分别投影到300维的子空间去。query里的每个word都对应一个300维的向量，一个query里会有多个向量，后面用sum求和操作得到一个汇总的300维向量，这是一个典型的Embedding操作。从图中可以看到，30k是指word字典的长度，300是embedding维数，30k\*300≈千万量级的参数。DSSM模型第一次探索了如何把大量稀疏的ID进行稠密表达的路径。

![](../../../../.gitbook/assets/v2-6687dc81bcc84a91afe3c47defb82514_r.jpg)

当然，DSSM模型本意不是用于广告预估问题。在深度学习最初引入CTR预估问题时，当时业界的一些公司如Google、百度等已经在探索如何把大量的稀疏数据进行降维的方法。一个典型的做法是用辅助的方式分两阶段进行：第一阶段，用FM模型把大量的稀疏ID学习到对应的embedding表达，跟DSSM模型类似，能够得到几百维的稠密向量。第二阶段是基于稠密的输入用多层全连接网络预测最后的目标。从今天的视角来看，这种两阶段的方式是不如整体的端到端模型的。这个思考点在2013年-2014年左右一直有人进行尝试，但当时一是因为深入学习框架的没能普及，二是对整个计算力的估计不足，因此没有达到比较好的进展，直到2016年左右，才有所突破，当然这里面很重要的一点是得益于优秀的深度学习框架如TensorFlow、MXNet等的开源和普及，进一步促进了整个工业界的思考和前进。

### GwEN

下面以阿里2016年的网格框架为例进行介绍。整个稀疏分组嵌入网络结构（GwEN）分为两部分，如下图左边所示。第一部分，把大规模的稀疏特征ID用Embedding操作映射为低维稠密的Embedding向量，然后把每个特征组的 Embedding进行简单的sum或average的pooling操作，得到Group-wise的Embedding向量。第二部分，多个特征组的向量通过Concatenate操作连接在一起，构成原始样本的完整稠密表达，喂给后续的全连接层。

![](../../../../.gitbook/assets/v2-d1c321730d698b2a797e9915d56125c1_r.jpg)

GwEN网络结构是比较基础的，但同样也非常重要。因为在最初大家普遍的直观思考是，假设有个高维的稀疏输入，典型的数量级是，然后将每个ID学习到一个表达，如果表达太小，便不足以客刻画信息本身，那么设想投影维度控制在数百上千估计是合适的。早期百度或Google的探索中，大概是一样的量级。这样算来，全空间便达到万亿的量级，极其恐怖，一方面对于训练样本的要求，另一方面对于背后的计算能力的要求都非常高。Group-wise Embedding的核心想法是借鉴CNN网络的局部感知野（Local Receptive Field）的思想。以整体输出表达向量为例，其实不需要每个ID达到千维的表达，因为在分组表示学习中每个特征组可以分别得到一个低维的表达，一共个分组，组里面的每个ID只需要学习的表达即可。这样，整个参数规模可以直接压缩到百亿的量级，这是工业界比较舒服的量级。尽管GwEN这种网络结构非常简单，但提出了非常重要的Group-wise Embedding 的概念，现在也称为求解大规模稀疏数据问题的通用网络单元。GwEN网格结构在2016年左右在阿里内部已经上线。

15年的时候，基于MLR的算法迭代进入瓶颈。当时认识到，要想进一步发挥MLR模型的非线性能力，需要提高模型的分片数——模型的参数相应地会线性增长，需要的训练样本量同样要大幅度增加，这不太现实。期间我们做了些妥协，从特征的角度进行优化，比如设计了一些直观的复合特征，典型的如”hit类特征”：用户历史浏览过商品A/B/C，要预估的广告是商品C，通过集合的”与”操作获得”用户历史上浏览过广告商品”这个特征。细心的读者应该很容易联想到后来我们进一步发展出来的DIN模型，通过类似attention的技巧拓展了这一方法。后来进一步引入一些高阶泛化特征，如user-item的PLSA分解向量、word2vec embedding等。但这些特征引入的代价大、收益低、工程架构复杂。

15年底16年初的时候我们开始认真地思考突破MLR算法架构的限制，向DL方向迈进。这个时间在业界不算最早的，原因如前所述，MLR是DL之前我们对大规模非线性建模思路的一个可行解，它助力了业务巨大的腾飞，因此当时够用了——能解决实际问题就是好武器，这很重要。在那个时间点，业界已经有了一些零散的DL建模思路出现，最典型的是B家早期的两阶段建模解法——先用LR/FM等把高维离散特征投影为数千规模的稠密向量，然后再训练一个MLP模型。我们最初也做过类似的尝试如w2v+MLR/DNN，但是效果不太显著，看不到打败MLR的希望\(不少团队从LR发展过来，这种两阶段建模打败LR应该是可行的\)。这里面关键点我们认为是端到端的建模范式。

![](../../../../.gitbook/assets/timline-jie-tu-20190326104120.png)

实践和思考不久催生了突破。16年5-6月份我构思出了第一代端到端深度CTR模型网络架构\(内部代号GwEN, group-wise embedding network\)，如上图所示。对于这个网络有多种解释，它也几乎成为了目前业界各个团队使用深度CTR模型最基础和内核的版本。图1给出了思考过程，应该说GwEN网络脱胎换骨于MLR模型，是我们对互联网尺度离散数据上端到端进行非线性建模的第二次算法尝试。当然跟大规模MLR时期一样，我们再一次遭遇了那三个关键挑战：

* 从数百维统计特征到数十亿离散特征，训练程序要做重大升级，从数据并行模式要升级到模型并行方式，且非线性模型复杂度高，需要充分利用数据的结构化特点进行加速；
* 在这种互联网尺度\(百亿参数&样本\)的数据上，模型能不能学习到兼具拟合能力与泛化能力的范式？
* 超大规模数据上的非凸优化\(MLR加入正则后进一步变成非光滑\)学术界鲜有先例。它的收敛性是一个巨大的问号。

### Wide & Deep Learning

与阿里同时期，Google推出了Wide & Deep Learning（WDL）模型，一个非常出名的模型。详细内容可以从论文中查询Cheng et al, “Wide & deep learning for recommender systems” 。WDL模型也非常简单，但巧妙的将传统的特征工程与深度模型进行了强强联合。Wide部分是指人工先验的交叉特征，通过LR模型的形式做了直接的预测。右边是Deep部分，与GwEN网络结构一样，属于分组的学习方式。WDL相当于LR模型与GwEN结合训练的网络结构。

![](../../../../.gitbook/assets/v2-02a9c1175b29356250a23c4c84fe2d5d_hd.jpg)

### FNN/PNN/DeepFM

GwEN和WDL是目前比较常用的模型，非常简单，所有后续人们继续做了很多改进，例如FNN，PNN以及DeepFM等。这些模型基础的部分与上面的GwEN和WDL模型类似，即Group-wise Embedding。改进的地方主要在后面的部分，引入了代数式的先验pattern，如FM模式，比较简单直接，可以给MLP 提供先验的结构范式。虽然理论上说，MLP可以表达任意复杂的分类函数，但越泛化的表达，拟合到具体数据的特定模式越不容易，也就是著名的“No Free Lunch”定理。因此代数式的先验结构引入确实有助于帮助MLP更好的学习。当然从另外的视角看，这种设定的结构范式比较简单，过于底层，也使得学习本身比较低效。

![](../../../../.gitbook/assets/v2-38c479b4a929455f9b6075a840370e66_r.jpg)

### DIN

另外一个工作是阿里在2017年发表的用户兴趣分布网络DIN模型。与上面的FNN,PNN等引入低阶代数范式不同，DIN的核心是基于数据的内在特点，引入了更高阶的学习范式。互联网上用户兴趣是多种多样的，从数学的角度来看，用户的兴趣在兴趣空间是一个多峰分布。在预测多兴趣的用户点击某个商品的概率时，其实用户的很多兴趣跟候选商品是无关的，也就是说我们只需要考虑用户跟商品相关的局部兴趣。所以DIN网络结构引入了兴趣局部激活单元，它受attention机制启发，从用户大量的行为集合中捕获到与candidate商品相关的行为子簇，对于用户行为子簇，通过Embedding操作，做weighted sum便可很好的预估出用户与candidate相关的兴趣度。传统的GwEN、WDL、FNN等模型在刻画用户兴趣分布时，会简单的将用户兴趣特征组做sum或average的pooling操作，这会把用户真正相关的兴趣淹没在pooling过程中。DIN模型的具体细节可以参考论文：Zhou et al, “Deep Interest Network for click-through rate prediction” 。

![](../../../../.gitbook/assets/v2-8c386a725488fd5013c37af0d052ffdd_r.jpg)

### DIEN

### ESMM

### DICM

## 建议与趋势探讨

大厂为追求最高的收益可以选用复杂的技术，尤其是像广告这样的部门，资源和人力投入的性价比超值。但对于小厂技术储备和投入相对不足，上面介绍的大量精细工作其实很难实施。模型架构层面一个可行的建议是：采用DQM式结构，把user/ad/query或上下文统一嵌入到vector空间，然后用向量计算架构进行在线服务。好处是在线预估系统可以极简，从而可以集中精力到离线的特征/模型调优，rocket/MTL等协同网络架构都是可以尝试的点，这可以保障轻松拿到业务效果的第一桶金；

如果是规模化的算法团队，愿意投入DL算法的设计，建议: 参考而绝不要盲信现有paper里面的架构，不要再把WDL/DIN等这类已有工作当成宝典。我的观点是，DL时代model这个词已经虚化了，像浅层模型时代LR/MLR等固定的模型已经不存在。模型是死的，场景是活的，遵循一定的规律、充分了解你的领域数据特点，吸收DIN/DIEN/ESMM这类方法的思考套路，定制适合具体问题的网络结构；

再进一步，如果是中大型有较强的实力掌控技术大盘的团队，建议牢记”算法-系统-架构”一体化的理念和方法论，DL对广告/推荐/搜索这类典型互联网应用系统的技术改造是全面而彻底的，现今的系统和架构大都是浅层模型时代遗留的产物，今天面临着复杂算法、异构硬件的多重冲刷，是时候打破旧规则，建立全新的基础设施；

特别聚焦到CTR预估技术上，离散特征的丰富性跟DL模型的效果密切相关，如果本身是容量很低的特征表达，模型是很难发挥的。例如我知道不少团队拿大量的低维统计特征为主的数据喂给DL模型，结果发现没什么效果，这显然是不得要领。“特征-模型-样本”是机器学习三要素，要时刻牢记。建议有实力有需求的团队，尽量充分地拓展更丰富的特征表达和样本信息容量，给模型创造更大的发挥空间；

以上所有建议都有一个重要的前提——自动化的算法迭代和生产链路。这件事在LR模型时代还没那么突出，因为算法迭代速度快不起来，但是在DL模型时代算法的开发和试错成本很低，完整的自动化链路才能真正发挥算法的威力，否则陷入各种在/离线手工胶水代码、人肉debug的汪洋大海，只能望洋兴叹了。

少重复造轮子，多拥抱开源。贡献开源或者从开源技术中吸取最新成果高起点迭代。打个硬广，我们最近刚把上述大部分技术统一整理集成为阿里开源项目X-DeepLearning\(XDL\)。XDL项目包含了我们对大规模分布式DL训练框架、各种实战自研模型\(囊括了前述大部分模型\)、高性能在线serving引擎为一体的工业级深度学习解决方案，感兴趣的同学可以了解下，开源项目地址：[https://github.com/alibaba/x-deeplearning](https://github.com/alibaba/x-deeplearning)

## Source

{% embed url="https://zhuanlan.zhihu.com/p/54822778" %}

{% embed url="https://mp.weixin.qq.com/s/MtnHYmPVoDAid9SNHnlzUw" %}

{% embed url="https://zhuanlan.zhihu.com/p/37562283" %}

{% embed url="https://zhuanlan.zhihu.com/p/34940250" %}



