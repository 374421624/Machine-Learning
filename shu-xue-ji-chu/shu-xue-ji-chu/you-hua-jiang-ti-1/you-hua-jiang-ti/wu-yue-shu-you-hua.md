# 无约束优化

## 梯度下降法\(Gradient descent\)

梯度下降法\(Gradient descent\)或最速下降法\(Steepest descent\)是求解无约束最优化问题的一种最常见的方法，有实现简单的有点。梯度下降法是迭代算法，每一步需要求解目标函数的梯度向量。

假设 $$f(x)$$ 是 $$R^n$$ 是具有一阶连续偏导数的函数，要求解的无约束最优化问题是：

                                                                        $$\mathop{min}\limits_{x\in R^n}f(x)$$ 

梯度下降法是一种迭代算法，选取适当的初值 $$x^{(0)}$$ ，不断迭代，更新 $$x$$ 的值，进行目标函数的极小化，直到收敛。由于[负梯度方向是使函数值下降最快的方向](https://zhuanlan.zhihu.com/p/33260455)，在迭代的每一步，以负梯度方向更新 $$x$$ 的值，从而达到减少函数值的目的。

由于$$f(x)$$具有一阶连续偏导数，若第 $$k$$ 次迭代值为 $$x^{(k)}$$ ，则可将 $$f(x)$$ 在 $$x^{(k)}$$ 附近进行一阶泰勒展开

                                                   $$f(x) = f(x^{(k)})+g_k^T(x-x^{(k)})$$ 

这里， $$g_k=g(x^{(k)}) = \nabla f(x^{(k)})$$ 为 $$f(x)$$ 在 $$x^{(k)}$$ 的梯度，求出第 $$k+1$$ 次迭代值 $$x^{(k+1)}$$ 

                                                            $$x^{(k)}+\lambda_kp_k \to x^{(k+1)}$$ 

其中， $$p_k$$ 是搜索方向，取负梯度方向 $$p_k=-\nabla f(x^{(k)})$$ ， $$\lambda_k$$ 是步长，由一维搜索确定，即 $$\lambda_k$$ 使得

                                             $$f(x^{(k)}+\lambda_kp_k) = \mathop{min}\limits_{\lambda\geq 0}f(x^{(k)}+\lambda p_k)$$ 

#### 具体算法如下

输入：目标函数 $$f(x)$$ ，梯度函数 $$g(x)=\nabla f(x)$$ ，计算精度 $$\varepsilon$$ ；输出： $$f(x)$$ 的极小点 $$x^*$$ 

1. \(1\) 取初始值 $$x^{(0)}\in R^n$$ ，置 $$k = 0$$ 
2. \(2\) 计算 $$f(x^{(k)})$$ 
3. \(3\) 计算梯度$$g_k=g(x^{(k)})$$：
4.     当$$||g_k||<\varepsilon$$时，停止迭代，令$$x^* = x^{(k)}$$；
5.     否则，令$$p_k=-g(x^{(k)})$$，求 $$\lambda_k$$ ，使 $$f(x^{(k)}+\lambda_kp_k)=\mathop{min}\limits_{\lambda\geq0}f(x^{(k)}+\lambda p_k)$$ 
6. \(4\) 置 $$x^{(k+1)}=x^{(k)}+\lambda_kp_k$$ ，计算 $$f(x^{(k+1)})$$ 
7.     当$$||f(x^{(k+1)})-f(x^{(k)})||<\varepsilon$$或$$||x^{(k+1)}-x^{(k)}||<\varepsilon$$时，停止迭代，令 $$x^*=x^{(k+1)}$$ 
8. \(5\) 否则，置 $$k = k+1$$ ，转\(3\)

当目标函数是凸函数时，梯度下降法的解是全局最优解。一般情况下，其解不保证是全局最优解，梯度下降法的收敛速度也未必是很快的。

## [牛顿法\(Newton's method\)](https://blog.csdn.net/itplus/article/details/21896453)

优化问题的最优解一般出现在函数的极值点上，也就 $$f'(x)=0$$ 的解，所以，牛顿法求解优化问题即求 $$f'(x)$$ 的零点。

首先，随机选择初始值 $$x_0$$，对于 函数 $$f(x)$$ ，计算相应的 $$f(x_0)$$ 和切线斜率 $$f'(x_0)$$\(这里 $$f'$$ 即表示 $$f$$ 的导数\)。然后我们计算穿过点 $$(x_0,f(x_0))$$ 并且斜率为 $$f'(x_0)$$ 的直线和 $$x$$ 轴的交点的 $$x$$ 坐标，也就是

                                                          $$0=f'(x_0)(x-x_0)+f(x_0)$$ 

我们将新求得的点的 $$x$$ 坐标命名为 $$x_1$$ ，通常 $$x_1$$ 会比 $$x_0$$ 更接近方程 $$f(x)=0$$ 的解（ $$f(x)=f(x_0)+(x-x_0)f'(x_0)$$ 处其实并不是完全相等的，而是近似相等，因为只做了一阶泰勒展开，当进行了无限阶泰勒展开全加起来才是等式。由于相对而言一阶变换量是最大的，所以我们先只考虑一阶展开）。因此我们可以利用 $$x_1$$ 开始下一轮迭代。迭代公式可简化为下式：

                                                         $$0=f'(x_0)(x-x_0)+f(x_0)$$ 

                                                    $$\Rightarrow -f(x_0)=f'(x_0)(x-x_0)$$ 

                                                    $$\Rightarrow -\frac{f(x_0)}{f'(x_0)}=(x-x_0)$$ 

                                                    $$\Rightarrow x_0-\frac{f(x_0)}{f'(x_0)}=x$$ 

                                                    $$\Rightarrow x_{n+1}=x_n-\frac{f(x_n)}{f'(x_n)}$$ 

通过迭代，这个式子必在 $$f(x^*) = 0$$ 的时候收敛，具体过程如下图。

![](../../../../.gitbook/assets/newtoniteration_ani.gif)

在优化问题中，其任务就是优化一个目标函数 $$f$$ ，求这个函数 $$f$$ 的极大极小问题，可以转化为求解函数的导数 $$f'=0$$ 的问题了。这就和上面说的牛顿求解很相似了。

为了求导数 $$f'=0$$ 的根，我们要把 $$f(x)$$ 在探索点 $$x_n$$ 处泰勒展开，展开到二阶泰勒近似，即要考虑导数的导数：

                                           $$f(x)=f(x_n)+f'(x_n)(x-x_n)+f''(x_n)\frac{(x-x_n)^2}{2}$$ 

我们考虑二阶导，即导数的导数：

                                                $$f'(x_{n+1})=f'(x_n)+f''(x_n)(x_{n+1}-x_n)=0$$ 

求得迭代公式：

                                                                     $$x_{n+1}=x_n-\frac{f'(x_n)}{f''(x_n)}$$ 

注意这里的 $$x$$ 是指的一阶的值，不是指的原函数 $$f(x)$$ 的 $$x$$ 。

上面解释了牛顿法的优化，都是以一维举例的（就一个变量 $$x$$ ）。从一维问题的迭代推广至多维问题只需将导数替换为梯度 $$\nabla f(x)$$ ，并将二阶导数的倒数替换为[Hessian矩阵](https://zh.wikipedia.org/wiki/%E6%B5%B7%E6%A3%AE%E7%9F%A9%E9%98%B5)的[逆矩阵](https://zh.wikipedia.org/wiki/%E9%80%86%E7%9F%A9%E9%98%B5) $$[Hf(x)]^{-1}$$ , 即：

                                             $$x_{n+1} = x_n-[Hf(x_n)]^{-1}\nabla f(x_n),\ n\geq 0$$ 

通常，使用牛顿法时会加入一个步长变量 $$\gamma\in(0,1)$$ 作微调，即：

                                                    $$x_{n+1} = x_n-\gamma[Hf(x_n)]^{-1}\nabla f(x_n)$$ 

这个方法即被称为无约束牛顿法, 通常用于第一步之后的迭代。

#### 统计学习方法（李航）牛顿法详解

考虑无约束最优化问题： $$\mathop{min}\limits_{x\in R^n}f(x)$$ 

假设$$f(x)$$具有二阶连续偏导数，若第 $$k$$ 次迭代值为 $$x^{(k)}$$ ，则可将 $$f(x)$$ 在 $$x^{(k)}$$ 附近进行二阶泰勒展开

                     $$f(x)=f(x^{(k)})+g^T_k(x-x^{(k)})+\frac{1}{2}(x-x^{(k)})^TH(x^{(k)})(x-x^{(k)})$$ 

这里， $$g_k=g(x^{(k)})=\nabla f(x^{(k)})$$ 是 $$f(x)$$ 的梯度向量在点 $$x^{(k)}$$ 的值， $$H(x^{(k)})$$ 是 $$f(x)$$ 的海赛矩阵

                                                             $$H(x)=[\frac{\partial^2f}{\partial x_i \partial x_j}]_{n\times n}$$ 

函数 $$f(x)$$ 有极值的必要条件是在极值点处一阶导数为 $$0$$ ，即梯度向量为 $$0$$ 特别是当 $$H(x^{(k)})$$ 是[正定矩阵](https://zh.wikipedia.org/wiki/%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5)时，函数 $$f(x)$$ 的极值为极小值。牛顿法利用极小值的必要条件 $$\nabla f(x)=0$$ ，每次迭代从点 $$x^{(k)}$$开始，求目标函数的极小点，作为第 $$k+1$$次迭代值 $$x^{(k+1)}$$ ，具体地，假设 $$x^{(k+1)}$$满足 $$\nabla f(x^{(k+1)}) = 0$$ 结合泰勒展开则有

                                                        $$\nabla f(x)=g_k+H_k(x-x^{(k)})$$ 

其中 $$H_k=H(x^{(k)})$$ ，这样

                                                         $$g_k+H_k(x^{(k+1)}-x^{(k)})=0$$ 

因此，

                                                            $$x^{(k+1)}=x^{(k)}-H^{-1}_kg_k$$ 

将上式作为迭代公式的算法就是牛顿法。

#### 具体算法如下

输入：目标函数 $$f(x)$$ ，梯度 $$g(x)=\nabla f(x)$$ ，海塞矩阵 $$H(x)$$ ，精度要求 $$\varepsilon$$ 

输出： $$f(x)$$ 的极小点 $$x^*$$ 

1. \(1\) 取初始值 $$x^{(0)}$$ ，置 $$k = 0$$ 
2. \(2\) 计算 $$g_k=g(x^{(k)})$$ 
3. \(3\) 若 $$||g_k||<\varepsilon$$ ，则停止计算，得近似解 $$x^*=x^{(k)}$$ 
4. \(4\) 计算 $$H_k=H(x^{(k)})$$ ，并求 $$p_k$$ ： $$H_kp_k=-g_k$$ 
5. \(5\) 置 $$x^{(k+1)}=x^{(k)}+p_k$$ 
6. \(6\) 置 $$k = k+1$$ ，转\(2\) 

## 拟牛顿法\(quasi-Newton method\)

在牛顿法的迭代中，需要计算海赛矩阵的逆矩阵，这一计算比较复杂，考虑用一个 $$n$$ 阶矩阵 $$G_k=G(x^{(k)})$$ 来近似替代 $$H^{-1}=H^{-1}(x^{(k)})$$ 。这就是拟牛顿法的基本想法。先看牛顿法迭代中海赛矩阵 $$H_k$$ 满足的条件。首先， $$H_k$$ 满足以下关系。在 $$\nabla f(x)=g_k+H_k(x-x^{(k)})$$ 中取 $$x = x^{(k)}$$ ，得

                                              $$g_{k+1}-g_k=H_k(x^{(k+1)}-x^{(k)})$$ 

记 $$y_k=g_{k+1}-g_k$$ ， $$\delta_k=x^{(k+1)}-x^{(k)}$$ ，则

                                                 $$y_k=H_k\delta_k$$ 或 $$H_k^{-1}y_k=\delta_k$$ 

上式即称为拟牛顿条件。如果 $$H_k$$ 是正定的\( $$H^{-1}_k$$也是正定的\)，那么可以保证牛顿法搜索方向 $$p_k$$ 是下降方向。这是因为搜索方向是 $$p_k=-H^{-1}_kg_k$$ ，结合牛顿法迭代式 $$x^{(k+1)}=x^{(k)}-H^{-1}_kg_k$$ 有

                                            $$x=x^{(k)}+\lambda p_k=x^{(k)}-\lambda H^{-1}_kg_k$$ 

所以 $$f(x)$$ 在 $$x^{(k)}$$ 的泰勒展开式可以近似写成

                                               $$f(x)=f(x^{(k)})-\lambda g^T_kH^{-1}_kg_k$$ 

因 $$H^{-1}_k$$ 正定，故有 $$g^T_kH^{-1}_kg_k>0$$ 当 $$\lambda$$ 为一个充分小的正数时，总有 $$f(x)<f(x^{(k)})$$ ，也就是说 $$p_k$$ 是下降方向。

拟牛顿法将 $$G_k$$ 作为 $$H^{-1}_k$$ 的近似，要求矩阵 $$G_k$$ 满足同样的条件。首先，每次迭代矩阵 $$G_k$$ 是正定的。同时， $$G_k$$ 满足下面的拟牛顿条件：

                                                              $$G_{k+1}y_k=\delta_k$$ 

按照拟牛顿条件选择 $$G_k$$ 作为 $$H^{-1}_k$$ 的近似或选择 $$B_k$$ 作为 $$H_k$$ 的近似的算法称为拟牛顿法。按照拟牛顿条件，在每次迭代中可以选择更新矩阵 $$G_{k+1}$$ ：

                                                         $$G_{k+1}=G_k+\Delta G_k$$ 

#### DFP

DFP算法选择 $$G_{k+1}$$ 的方法是，假设每一步迭代中矩阵 $$G_{k+1}$$ 是由 $$G_k$$ 加上两个附加项构成的，即

                                                       $$G_{k+1}=G_k+P_k+Q_k$$ 

其中 $$P_k,\ Q_k$$ 是待定矩阵。这时，

                                                $$G_{k+1}y_k=G_ky_k+P_ky_k+Q_ky_k$$ 

为使 $$G_{k+1}$$ 满足拟牛顿条件，可使 $$P_k$$ 和 $$Q_k$$ 满足：

                                                $$P_ky_k=\delta_k$$           $$Q_ky_k=-G_ky_k$$ 

事实上，不难找出这样的 $$P_k$$ 和 $$Q_k$$ ，例如取

                                                 $$P_k=\frac{\delta_k\delta_k^T}{\delta_k^Ty_k}$$           $$Q_k = -\frac{G_ky_ky^T_kG_k}{y_k^TG_ky_k}$$ 

这样就可得到矩阵 $$G_{k+1}$$ 的迭代公式：

                                                    $$G_{k+1}=G_k+\frac{\delta_k\delta_k^T}{\delta_k^Ty_k} -\frac{G_ky_ky^T_kG_k}{y_k^TG_ky_k}$$ 

称为DFP算法。可以证明，如果初始矩阵 $$G_0$$ 是正定的，则迭代过程中的每个矩阵 $$G_k$$ 都是正定的。

1. 输入：目标函数 $$f(x)$$ ，梯度 $$g(x)=\nabla f(x)$$ ，精度要求 $$\varepsilon$$ ；输出： $$f(x)$$ 的极小点 $$x^*$$ 
2. \(1\) 选定初始点 $$x^{(0)}$$ ，取 $$G_0$$ 为正定对称矩阵，置 $$k = 0$$ 
3. \(2\) 计算 $$g_k=g(x^{(k)})$$ 若 $$||g_k||<\varepsilon$$ ，则停止计算，得近似解 $$x^*=x^{(k)}$$ 
4. \(3\) 置 $$p_k=-G_kg_k$$ 
5. \(4\) 一维搜索：求 $$\lambda_k$$ 使得 $$f(x^{(k)}+\lambda_kp_k)=\mathop{min}\limits_{\lambda\geq0}f(x^{(k)}+\lambda p_k)$$ 
6. \(5\) 置 $$x^{(k+1)}=x^{(k)}+\lambda_kp_k$$ 
7. \(6\) 计算 $$g_{k+1}=g(x^{(k+1)})$$ 
8.          若 $$||g_k||<\varepsilon$$ ，则停止计算，得近似解 $$x^*=x^{(k+1)}$$ ；
9.          否则，按 $$G_{k+1}=G_k+\frac{\delta_k\delta_k^T}{\delta_k^Ty_k} -\frac{G_ky_ky^T_kG_k}{y_k^TG_ky_k}$$ 计算出 $$G_{k+1}$$ 
10. \(7\) 置 $$k = k+1$$ ，转\(3\)

#### BFGS

BFGS算法是最流行的拟牛顿算法。可以考虑用 $$G_k$$ 逼近海赛矩阵的逆矩阵 $$H^{-1}$$ ，也可以考虑用 $$B_k$$ 逼近海赛矩阵 $$H$$ 。这时，相应的拟牛顿条件是

                                                                  $$B_{k+1}\delta_k=y_k$$ 

可以用同样的方法得到另一迭代公式。首先令

                                                         $$B_{k+1}=B_k+P_k+Q_k$$ 

                                                $$B_{k+1}\delta_k=B_k\delta_k+P_k\delta_k+Q_k\delta_k$$ 

考虑使 $$P_k$$ 和 $$Q_k$$ 满足：

                                                  $$P_k\delta_k=y_k$$       $$Q_k\delta_k=-B_k\delta_k$$ 

找出适合条件的 $$P_k$$ 和 $$Q_k$$，得到BFGS算法矩阵 $$B_{k+1}$$ 的迭代公式：

                                                   $$B_{k+1}=B_k+\frac{y_ky_k^T}{y^T_k\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}$$ 

可以证明，如果初始矩阵 $$B_0$$ 是正定的，则迭代过程中的每个矩阵 $$B_k$$ 都是正定的。

1. 输入：目标函数 $$f(x)$$ ，梯度 $$g(x)=\nabla f(x)$$ ，精度要求 $$\varepsilon$$ ；输出： $$f(x)$$ 的极小点 $$x^*$$ 
2. \(1\) 选定初始点 $$x^{(0)}$$ ，取 $$B_0$$ 为正定对称矩阵，置 $$k = 0$$ 
3. \(2\) 计算 $$g_k=g(x^{(k)})$$ 若 $$||g_k||<\varepsilon$$ ，则停止计算，得近似解 $$x^*=x^{(k)}$$ 
4. \(3\) 由 $$B_kp_k = -g_k$$ 求出 $$p_k$$ 
5. \(4\) 一维搜索：求 $$\lambda_k$$ 使得 $$f(x^{(k)}+\lambda_kp_k)=\mathop{min}\limits_{\lambda\geq0}f(x^{(k)}+\lambda p_k)$$ 
6. \(5\) 置 $$x^{(k+1)} = x^{(k)}+\lambda_k p_k $$ 
7. \(6\) 计算 $$g_{k+1} = g(x^{(k+1)})$$ 
8.         若 $$||g_{k+1}||<\varepsilon$$ ，则停止计算，得近似解 $$x^*=x^{(k+1)}$$ 
9.         否则，按 $$B_{k+1}=B_k+\frac{y_ky_k^T}{y^T_k\delta_k}-\frac{B_k\delta_k\delta_k^TB_k}{\delta_k^TB_k\delta_k}$$ 计算出 $$B_{k+1}$$ 
10. \(7\) 置 $$k = k+1$$ ，转\(3\)

## 坐标下降法\(Coordinate descent\)

坐标下降法是一种非梯度优化方法，它在每步迭代中沿一个坐标方向进行搜索，通过循环使用不用的坐标方向来达到目标函数的局部极小值。

假设我们求解 $$f(x)$$ 的极小值，其中 $$x=(x_1,x_2,\dots,x_d)^T\in \mathbb{R}^d$$ 是一个 $$d$$ 维向量。从初始点 $$x^0$$ 开始，坐标下降法通过迭代地构造序列 $$x^0,x^1,x^2,\dots$$ 来解决问题， $$x^{t+1}$$ 的第 $$i$$ 个分量 $$x_i^{t+1}$$ 构造为：

                               $$x_i^{t+1}=\mathop{argmin}\limits_{y\in\mathbb{R}}f(x_1^{t+1},\dots,x_{i-1}^{t+1},y,x^t_{i+1},\dots,x_d^t)$$ 

通过执行此操作，显然有

                                               $$f(x^0)\geq f(x^1) \geq f(x^2)\geq\dots$$ 

与梯度下降法类似，通过迭代执行该过程，序列 $$x^0,x^1,x^2,\dots$$ 能收敛到所期望的局部极小点或驻点。坐标下降法不需计算目标函数的梯度，在每次迭代中仅需求解一维搜索问题，对于某些复杂问题计算较为简便。但若目标函数不光滑，则坐标下降法有可能陷入非驻点。

## 最小二乘法\(Least squares\)

最小二乘法是无约束的数学优化方法，通过最小化误差的平方和寻找数据的最佳函数匹配。利用最小二乘法可以简便地求得未知的数据，并使得这些求得的数据与实际数据之间误差的平方和为最小，即

                                                           $$\mathop{min}\limits_\theta \sum_{i=1}^n(\hat{y}-y_i)^2$$ 

 最小值可以通过对上式参数分别求[偏导数](https://zh.wikipedia.org/wiki/%E5%81%8F%E5%AF%BC%E6%95%B0)，然后使它们等于零得到。

#### Example

某次实验得到了四个数据点 $$(x,y)$$ ： $$(1,6)、(2,5)、(3,7)、(4,10)$$ 。我们希望找出一条和这四个点最匹配的直线 $$y = \beta_1+\beta_2 x$$， 即找出在“最佳情况”下能够大致符合如下超定线性方程组的 $$\beta_1$$ 和 $$\beta_2$$：

                 $$\beta_1+1\beta_2 = 6$$       $$\beta_1+2\beta_2 = 5$$       $$\beta_1+3\beta_2 = 7$$       $$\beta_1+4\beta_2 = 10$$ 

最小二乘法采用的手段是尽量使得等号两边的方差最小，也就是找出这个函数的最小值：

$$S(\beta_1,\beta_2) = [6-(\beta_1+1\beta_2)]^2+[5-(\beta_1+2\beta_2)]^2+[7-(\beta_1+3\beta_2)]^2+[10-(\beta_1+4\beta_2)]^2$$

最小值可以通过对 $$S(\beta_1,\beta_2)$$ 分别求 $$\beta_1$$ 和 $$\beta_2$$ 的偏导数，然后使它们等于零得到。

                     $$\frac{\partial S}{\partial\beta_1} = 0 =8\beta_1+20\beta_2-56$$        $$\frac{\partial S}{\partial\beta_2} = 0 =20\beta_1+60\beta_2-154$$ 

如此就得到了一个只有两个未知数的方程组，易得 $$\beta_1=3.5,\ \beta_2 = 1.4$$ ，所以 $$y = 3.5+1.4x$$ 最佳

## 置信域方法（Trust-region methods）

置信域方法（Trust-region methods）又称为信赖域方法，它是一种最优化方法，能够保证最优化方法总体收敛。

