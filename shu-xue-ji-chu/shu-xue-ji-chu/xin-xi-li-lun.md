# 信息理论

信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。信息论的基本想法是一个不太可能发生的事件居然发生了，要比一个非常可能发生的事件发生，能提供更多的信息。消息说“今天早上太阳升起”，信息量是如此之少，但一条消息说“今天早上有日食”，信息量就很丰富了。我们想通过这种基本想法来量化信息。特别是：

* 1、非常可能发生的事件信息量比较少，并且极端情况下，确保能够发生的事件应该没有信息量。
* 2、较不可能发生的事件具有更高的信息量。
* 3、独立事件应具有增量的信息。比如投掷的硬币两次正面朝上传递的信息量，应是投掷一次正面朝上
*       的信息量的两倍。

## 自信息\(Self-information\)

定义一个事件的自信息为： $$I(x)=-\log P(x)$$ 

以 $$e$$ 为底的对数单位是奈特，一奈特是以 $$\frac{1}{e}$$ 的概率观测到一个事件时获得的信息量。

以 $$2$$ 为底的对数单位是比特或香农。

## 香农熵\(Shannon entropy\)

自信息只处理单个的输出。我们可以用香农熵来对整个概率分布中的不确定性总量进行量化：

                               $$H(x)=\mathbb{E}_{x\sim P}[I(x)]=-\mathbb{E}_{x\sim P}[\log P(x)]$$     \(不明确说的话都是 $$e$$ 为底\)

也记作 $$H(P)$$ 。换言之，一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。



## KL散度\(Kullback-Leibler divergence\)

## 交叉熵\(Cross-entropy\)





