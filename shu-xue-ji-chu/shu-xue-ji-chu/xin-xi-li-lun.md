# 信息理论

信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。信息论的基本想法是一个不太可能发生的事件居然发生了，要比一个非常可能发生的事件发生，能提供更多的信息。消息说“今天早上太阳升起”，信息量是如此之少，但一条消息说“今天早上有日食”，信息量就很丰富了。我们想通过这种基本想法来量化信息。特别是：

* 1、非常可能发生的事件信息量比较少，并且极端情况下，确保能够发生的事件应该没有信息量。
* 2、较不可能发生的事件具有更高的信息量。
* 3、独立事件应具有增量的信息。比如投掷的硬币两次正面朝上传递的信息量，应是投掷一次正面朝上
*       的信息量的两倍。

## 自信息\(Self-information\)

定义一个事件的自信息为： $$I(x)=-\log P(x)$$ 

以 $$e$$ 为底的对数单位是奈特，一奈特是以 $$\frac{1}{e}$$ 的概率观测到一个事件时获得的信息量。

以 $$2$$ 为底的对数单位是比特或香农。

## 香农熵\(Shannon entropy\)

自信息只处理单个的输出。我们可以用香农熵来对整个概率分布中的不确定性总量进行量化：

                               $$H(x)=\mathbb{E}_{x\sim P}[I(x)]=-\mathbb{E}_{x\sim P}[\log P(x)]$$     \(不明确说的话都是 $$e$$ 为底\)

也记作 $$H(P)$$ 。换言之，一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量。

那些接近确定性的分布（输出几乎可以确定）具有较低熵；那些接近均匀分布的概率分布具有较高的熵。

## KL散度\(Kullback-Leibler divergence\)

如果对于同一个随机变量 $$x$$ 有两个单独的概率分布 $$P(x)$$ 和 $$Q(x)$$ ，可以使用KL散度来衡量这两个分布的差异：

                           $$D_{KL}(P||Q)=\mathbb{E}_{x\sim P}[\log \frac{P(x)}{Q(x)}]=\mathbb{E}_{x\sim P}[\log P(x)-\log Q(x)]$$ 

## 交叉熵\(Cross-entropy\)

一个和KL散度密切联系的量是交叉熵，即 $$H(P,Q)=H(P)+D_{KL}(P||Q)$$ ，与KL散度很像，但是缺少左边一项：

                                                          $$H(P,Q)=-\mathbb{E}_{x\sim P}\log Q(x)$$ 



