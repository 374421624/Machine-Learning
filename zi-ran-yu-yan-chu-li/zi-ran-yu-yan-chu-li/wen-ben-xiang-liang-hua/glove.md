# GloVe

除了word2vec，LSA、PLSA、LDA等传统的主题模型也可用于生成词向量，这种模型对全局的统计信息（比如共现频率、主题等）应用较充分，但是很难体现词与词之间的线性关系（比如King-Man=Queen-Woman）。而word2vec能够较好地体现词与词之间的线性关系，但是对全局的统计信息利用不足。GloVe提出的初衷就是为了充分吸收两者的长处。实质上，GloVe依然是一种矩阵分解方法，非常类似于早期的LFM（Latent Factor Model），只是分解对象变为了共现频率的对数，同时对高低频做了一定的权重调整

GloVe的英文全称为Global Vector for Word Representation，即单词表示的全局向量，是一种获取单词向量表示的非监督学习方法。GloVe模型的损失函数为：

                                                 $$\text{Loss}=\frac{1}{2}\sum\limits_{i,j=1}^Vf(P_{ij})(w_i\tilde{w}_j-\log P_{ij})$$ 

其中 $$P_{ij}$$ 表示单词 $$i$$ 和 $$j$$ 共现的频率，对于单词共现，其共现概率记为 $$\frac{1}{d}$$ ， $$d$$ 为窗口中的距离； $$f(P_{ij})$$ 表示对权重的调整； $$\log P_{ij}$$ 也是对高频的降权，以免高频共现词对过度支配模型。实际上模型训练了两套参数： $$W$$ 及 $$\tilde{W}$$ ，如果共现矩阵 $$P$$ 是对称的，则 $$W = \tilde{W}$$ 。一般来说，共现是对称的，即单词 $$i$$ 和 $$j$$ 共现，频率计数对两者来说是一样的，但考虑到模型可能过拟合，一般最终的输出去两者之和或平均。

