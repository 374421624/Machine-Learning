# 求解增强学习问题

在介绍完增强学习的基本框架和其中几个关键问题后，我们开始求解增强学习的问题。这里主要介绍一些经典的求解方法，这些方法可以分成两类：已知环境模型和未知环境模型。

对于已知环境模型的问题，我们将介绍迭代法（Policy Iteration）和价值迭代法（Value Iteration），它们被统称为动态规划。

对未知环境模型的问题，我们将介绍蒙特卡罗法（Monte Carlo）和时序差分法（Temporal Differential）。

## 已知环境模型的问题

### Agent的目标

由于动态规划法需要知道环境模型的具体信息，因此在这里我们需要对模型做出定义。Agent在发出动作前已经知道了如下信息：

* 模型的状态转移概率 $$p(S_{t+1}=s'|S_t=s,A_t=a)$$ 
* 模型的奖励函数 $$r(s,a,s')=E[R_{t+1}|A_t=a,S_t=s,S_{t+1}=s']$$ 

根据上面的信息求解出最优策略 $$\pi(A_t=a|S_t)$$ 。当然，在前面的章节我们已经论证过，当获得了最优的价值函数后，就可以找到最优策略，这个过程可能会比较耗时，但是一定可以实现。我们将利用这个性质进行求解。

### 策略迭代法（Policy Iteration）

策略迭代法过程可以拆解成两个部分：策略评估（Policy Evaluation）和策略改进（Policy Improvement）

#### 策略评估



#### 策略改进

#### 策略迭代

### 价值迭代（Value Iteration）

## 未知环境模型的问题

### 蒙特卡罗法

### 时序差分法

