# 增强学习的数学表达形式

在了解了增强学习的大致框架之后，我们继续深入探讨增强学习的数学形态。一个增强学习系统可以抽象成马尔可夫决策过程（Markov Decision Process，MDP）的模式，其中还包括策略函数（Policy Function）、奖励（Reward）与回报（Return）、价值函数（Value Function）这几个核心部分。前面我们已经介绍了增强学习的概念，如果希望采用数学的方式来描述这个交互过程，则需要解决下面的问题：

* Agent的动作是根据什么做出的？
* 这个部分是否可以通过模型表达出来？环境是如何随Agent的动作变化的？
* 这里面是不是包含着某种规律？如果有规律，那么能不能用模型把它表达出来？

## MDP

对于大多数的增强学习问题，我们都可以用马尔可夫决策过程（MDP）来描述。MDP这个概念可以分两个方面进行介绍。这里假设Agent在每一个状态下的动作是有限的，同时环境的状态的数量也是有限的。首先是交互过程的马尔可夫特征。我们知道增强学习的过程是Agent和Environment交互的过程，当Agent的Action产生后，Environment会切换到下一个状态，那么这个新状态的产生会和哪些因素有关呢？直觉上看，新状态 $$S_{t+1}$$ 会与 $$S_t$$ 和 $$A_t$$ 相关。那么除此之外，会不会和 $$S_{t-1}$$ 和 $$A_{t-1}$$ 相关呢？拥有马尔可夫特性的问题告诉我们，不会。马尔可夫特性将状态转移限定在只和前一个状态相关，也就是说：

                         $$P\{R_{t+1}=r,S_{t+1}=s'|S_0,A_0,R_1,\cdots,S_{t-1},A_{t-1},R_t,S_t,A_t\}$$ 

相当于：

                                                     $$P\{R_{t+1}=r,S_{t+1}=s'|S_t,A_t\}$$ 

这个特性极大地简化了问题的复杂度，使得整个问题变得更加清晰。同时，状态间相互依赖的特性依然保留，使得这个特性仍然可以保留问题本身最核心的一些特点。

其次是决策过程。当Agent接收到某一时刻 $$t$$ 的 $$S_t$$ 和 $$R_t$$ 之后，它将根据自身的策略做出相应的 $$A_t$$ ，Environment接收到Action后，会根据 $$S_t$$ 和 $$A_t$$ 转换到下一个状态 $$S_{t+1}$$ ，转换的方式依赖于 $$p(s_{t+1}|s_t,a_t)$$ 概率，同时给出相对应的奖励 $$R_{t+1}=r(s_t,a_t,s_{t+1})$$ 。之后Agent再做出选择。如此不断进行的状态转换构成了环境状态和Agent动作的序列，这个序列就可以描述过程。

## 策略函数

## 奖励与回报

## 价值函数

## 贝尔曼方程

## 最优策略性质

