# 已知环境模型的问题

## Agent的目标

由于动态规划法需要知道环境模型的具体信息，因此在这里我们需要对模型做出定义。Agent在发出动作前已经知道了如下信息：

* 模型的状态转移概率 $$p(S_{t+1}=s'|S_t=s,A_t=a)$$ 
* 模型的奖励函数 $$r(s,a,s')=E[R_{t+1}|A_t=a,S_t=s,S_{t+1}=s']$$ 

根据上面的信息求解出最优策略 $$\pi(A_t=a|S_t)$$ 。当然，在前面的章节我们已经论证过，当获得了最优的价值函数后，就可以找到最优策略，这个过程可能会比较耗时，但是一定可以实现。我们将利用这个性质进行求解。

## 策略迭代法（Policy Iteration）

策略迭代法过程可以拆解成两个部分：策略评估（Policy Evaluation）和策略改进（Policy Improvement）

### 策略评估

策略评估是指从策略到价值函数的推导。也就是说，对于某个策略 $$\pi$$ ，求出利用这个策略所产生的价值函数 $$v_\pi(s)$$ 。利用贝尔曼方程，可以推导出求解价值函数的公式：

                                     $$v_\pi(s)=E_\pi[R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots|S_t=s]$$ 

                                                  $$= E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]$$ 

                                                 $$= \sum\limits_a\pi(a|s)\sum\limits_{s'}p(s'|s,a)[r(s,a,s')+\gamma v_\pi(s')]$$ 

由于我们假设已经知道了环境模型的全部信息，所以在上面的公式中唯一未知的就是价值函数。求解这个公式有两种方法：一种是求公式的闭式解，也就是把它当成一个方程组去求解。如果模型中有 $$|S|$$ 个状态，那么这个方程组中就有 $$|S|$$ 个未知数。另一种是采用迭代的方式求解，由于上式中的 $$\gamma<1$$ ，所以价值函数最终会收敛。因此可以将价值函数初始化成 $$0$$ ，然后反复利用上面的公式进行迭代，直到价值函数收敛为止。关于判断价值函数收敛的方法，我们可以比较前后两轮价值函数数值的差距，当差距小于某一个确定的值时，就认为价值函数已经收敛。有关迭代式的价值评估可以用下面的算法来进行计算。

### 策略改进

### 策略迭代

## 价值迭代（Value Iteration）

## 策略函数和价值函数关系

