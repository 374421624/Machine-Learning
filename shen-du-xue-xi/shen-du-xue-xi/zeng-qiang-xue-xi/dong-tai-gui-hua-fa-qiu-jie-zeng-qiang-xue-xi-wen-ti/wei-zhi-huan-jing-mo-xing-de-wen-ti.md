# 未知环境模型的问题

对于绝大多数现实中的增强学习问题，Agent只能在采取动作之后获得下一个状态以及对应的奖励，对其中的概率转移函数和奖励函数的细节无法知晓。这一类算法也被称为无模型（Model-Free）算法。当然，没有模型不代表无法进行求解，我们可以通过采样的方法进行求解，也就是具有增强学习特色的学习方式——在实践中学习，当模型通过实践采样到大量的与环境交互的数据之后，它就可以利用这些数据求出价值函数的模型，求解在当前数据下的最优策略了。在无模型算法中，比较经典的有蒙特卡罗法和时序差分法。

## 蒙特卡罗法

在很多增强学习问题中，我们发现想获得环境模型完整的状态转移概率比较困难，但是获取某一步动作后的状态和奖励序列是比较容易的。比较典型的例子就是电子游戏，一般来说，玩家想要把每一个场景（也可以称作状态）下经过任意操作后产生的状态完全收集到是不太现实的，但是游戏会自然而然地为玩家展现出它的下一个场景和奖励。

## 时序差分法

### Q-Learning

