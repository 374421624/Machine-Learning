# 展开计算图

计算图是形式化一组计算结构的方式，如那些涉及将输入和参数映射到输出和损失的计算。我们对展开递归或循环计算得到的重复结构进行解释，这些重复结构通常对应于一个事件链。展开这个计算图将导致深度网络结构中的参数共享。例如，考虑动态系统的经典形式：

                                                                    $$s^{(t)}=f(s^{(t-1)};\theta)$$ 

其中 $$s^{(t)}$$ 称为系统的状态。 $$s$$ 在时刻 $$t$$ 的定义需要参考时刻 $$t-1$$ 时同样的定义。因此上式是循环的。对有限时间步 $$\tau$$ ， $$\tau-1$$ 次应用这个定义可以展开这个图。例如 $$\tau=3$$ ，我们对上式展开，可以得到

                                                    $$s^{(3)}=f(s^{(2)};\theta)=f(f(s^{(1)};\theta);\theta)$$ 

以这种方式重复应用定义，展开等式，就能得到不涉及循环的表达。现在我们可以使用传统的有向无环图呈现这样的表达，如下图

![](../../../.gitbook/assets/screenshot-from-2018-12-29-19-10-19.png)

作为另一个例子，让我们考虑由外部信号 $$x^{(t)}$$ 驱动的动态系统。

                                                                $$s^{(t)}=f(s^{(t-1)},x^{(t)};\theta)$$ 

我们可以看到，当前状态包含了整个过去序列的信息。

循环神经网络可以通过许多不同的方式建立。就像几乎所有函数都可以被认为是前馈网络，本质上任何涉及循环的函数都可以视为一个循环神经网络。很多循环神经网络使用下式或类似的公式定义隐藏单元的值。为了表明状态是网络的隐藏单元，我们使用变量 $$h$$ 代表状态重写式 $$s^{(t)}=f(s^{(t-1)},x^{(t)};\theta)$$ ：

                                                                $$h^{(t)}=f(h^{(t-1)},x^{(t)};\theta)$$ 

如下图所示，典型RNN会增加额外的架构特性，如读取状态信息 $$h$$ 进行预测的输出层。

![](../../../.gitbook/assets/screenshot-from-2018-12-29-19-23-30.png)

当训练循环网络根据过去预测未来时，网络通常要学会使用 $$h^{(t)}$$ 





