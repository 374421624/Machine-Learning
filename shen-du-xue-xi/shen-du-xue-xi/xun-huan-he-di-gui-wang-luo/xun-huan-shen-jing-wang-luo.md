# 循环神经网络

基于图展开和参数共享的思想，我们可以设计各种循环神经网络。循环神经网络中一些重要的设计模式包括以下几种：

![&#x6BCF;&#x4E2A;&#x65F6;&#x95F4;&#x6B65;&#x90FD;&#x6709;&#x8F93;&#x51FA;&#xFF0C;&#x5E76;&#x4E14;&#x9690;&#x85CF;&#x5355;&#x5143;&#x4E4B;&#x95F4;&#x6709;&#x5FAA;&#x73AF;&#x8FDE;&#x63A5;&#x7684;&#x5FAA;&#x73AF;&#x7F51;&#x7EDC;&#x3002;](../../../.gitbook/assets/timline-jie-tu-20190102141413%20%282%29.png)

![&#x6BCF;&#x4E2A;&#x65F6;&#x95F4;&#x6B65;&#x90FD;&#x4EA7;&#x751F;&#x4E00;&#x4E2A;&#x8F93;&#x51FA;&#xFF0C;&#x53EA;&#x6709;&#x5F53;&#x524D;&#x65F6;&#x523B;&#x7684;&#x8F93;&#x51FA;&#x5230;&#x4E0B;&#x4E2A;&#x65F6;&#x523B;&#x7684;&#x9690;&#x85CF;&#x5355;&#x5143;&#x4E4B;&#x95F4;&#x6709;&#x5FAA;&#x73AF;&#x8FDE;&#x63A5;&#x7684;&#x5FAA;&#x73AF;&#x7F51;&#x7EDC;&#x3002;](../../../.gitbook/assets/timline-jie-tu-20190102141837.png)

![&#x9690;&#x85CF;&#x5355;&#x5143;&#x4E4B;&#x95F4;&#x5B58;&#x5728;&#x5FAA;&#x73AF;&#x8FDE;&#x63A5;&#xFF0C;&#x4F46;&#x8BFB;&#x53D6;&#x6574;&#x4E2A;&#x5E8F;&#x5217;&#x540E;&#x4EA7;&#x751F;&#x5355;&#x4E2A;&#x8F93;&#x51FA;&#x7684;&#x5FAA;&#x73AF;&#x7F51;&#x7EDC;&#x3002;](../../../.gitbook/assets/timline-jie-tu-20190102141949%20%281%29.png)

## 前向传播

现在我们研究每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络的前向传播公式，即下图

![](../../../.gitbook/assets/timline-jie-tu-20190102141413%20%283%29.png)

这幅图描述了在序列索引号 $$t$$ 附近RNN的模型。其中：

（1） $$x^{(t)}$$ 代表在序列索引号 $$t$$ 时训练样本的输入。同样的， $$x^{(t-1)}$$ 和 $$x^{(t+1)}$$ 代表在序列索引号 $$t-1$$和 $$t+1$$ 时训练样本的输入。

（2） $$h^{(t)}$$ 代表在序列索引号 $$t$$ 时模型的隐藏状态。 $$h^{(t)}$$ 是由 $$x^{(t)}$$ 和 $$h^{(t-1)}$$ 共同决定。

（3） $$o^{(t)}$$ 代表在序列索引号 $$t$$ 时模型的输出。 $$o^{(t)}$$ 只由模型当前的隐藏状态 $$h^{(t)}$$ 决定。

（4） $$L^{(t)}$$ 代表在序列索引号 $$t$$ 时模型的损失函数。

（5） $$y^{(t)}$$ 代表在序列索引号 $$t$$ 时训练样本序列的真实输出。

（6） $$U, W, V$$ 这三个矩阵是我们的模型的线性关系参数，它在整个RNN网络中是共享的。也正因为是共享的，它体现了RNN的模型的“循环反馈”的思想。

有了上面的模型，RNN的前向传播算法就很容易得到了：

对于任意一个序列索引号 $$t$$ ，我们隐藏状态 $$h^{(t)}$$ 由 $$x^{(t)}$$ 和 $$h^{(t-1)}$$ 得到：

                                              $$h^{(t)}=\sigma(z^{(t)})=\sigma(Ux^{(t)}+Wh^{(t-1)}+b)$$ 

其中 $$\sigma$$ 为RNN的激活函数，一般为tanh， $$b$$ 为线性关系的偏倚。

序列索引号 $$t$$ 时的模型的输出 $$o^{(t)}$$ 的表达式比较简单：

                                                                   $$o^{(t)}=Vh^{(t)}+c$$ 

在最终在序列索引号 $$t$$ 时我们的预测输出为：

                                                                      $$\hat{y}^{(t)}=\sigma'(o^{(t)})$$ 

通常RNN是分类模型，所以上面这个激活函数 $$\sigma'$$ 一般是softmax。

通过损失函数 $$L^{(t)}$$ ，比如对数似然损失函数，我们可以量化模型在当前位置的损失，即 $$\hat{y}^{(t)}$$ 和 $$y^{(t)}$$ 差距

## 反向传播

RNN反向传播算法的思路和DNN是一样的，即通过梯度下降法一轮轮的迭代，得到合适的RNN模型参数 $$U,W,V,b,c$$ 。由于我们是基于时间反向传播，所以RNN的反向传播有时也叫做BPTT（back-propagation through time）。当然这里的BPTT和DNN也有很大的不同点，即这里所有的 $$U,W,V,b,c$$ 在序列的各个位置是共享的，反向传播时我们更新的是相同的参数。

为了简化描述，这里的损失函数我们为对数损失函数，输出的激活函数为softmax函数，隐藏层的激活函数为tanh函数。对于RNN，由于我们在序列的每个位置都有损失函数，因此最终的损失 $$L$$ 为：

                     

## Source

{% embed url="https://blog.csdn.net/anshuai\_aw1/article/details/85163572" %}



