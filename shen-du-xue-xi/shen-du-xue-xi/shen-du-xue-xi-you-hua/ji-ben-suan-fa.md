# 基本算法

## 批梯度下降

Vanilla梯度下降法，又称为批梯度下降法（batch gradient descent），在整个训练数据集上计算损失函数关于参数 $$\theta$$ 的梯度。因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。

对于给定的迭代次数，首先，我们利用全部数据集计算损失函数关于参数向量的梯度向量。然后，我们利用梯度的方向和学习率更新参数，学习率决定我们将以多大的步长更新参数。对于凸误差函数，批梯度下降法能够保证收敛到全局最小值，对于非凸函数，则收敛到一个局部最小值。

## 随机梯度下降\(SGD\)

随机梯度下降\(SGD\)及其变种很可能是一般机器学习中应用最多的优化算法，特别是在深度学习中。与批梯度下降使用全部数据不同，随机梯度下降按照数据生成分布抽取 $$m$$ 个小批量（独立同分布的）样本，通过它们梯度均值，我们可以得到梯度的无偏估计。

#### 算法过程

初始化：学习率 $$\epsilon_k$$ ，初始参数 $$\theta$$ 

* while 停止准则为满足 do：
*            从训练集中采包含 $$m$$ 个样本 $$\{x^{(1)},\dots,x^{(m)}\}$$ 的小批量，其中 $$x^{(i)}$$ 对应目标为 $$y^{(i)}$$ 
*            计算梯度估计： $$\hat{g}\gets \frac{1}{m}\nabla_\theta\sum\limits_i^m L(f(x^{(i)};\theta),y^{(i)})$$ 
*            应用更新：$$\theta \gets \theta- \epsilon\hat{g}$$ 

SGD算法中的一个关键参数是学习率。之前，我们介绍的SGD使用固定的学习率。在实践中，有必要随着时间的推移降低学习率，因此我们将第 $$k$$ 步迭代的学习率记作 $$\epsilon_k$$ 。

这是因为SGD中梯度估计引入的噪声源（ $$m$$ 个训练样本的随机采样）并不会在极小点处消失。相比之下，当我们使用批量梯度下降到极小点时，整个代价函数的真实梯度会变得很小，之后为 $$0$$ ，因此批量梯度下降可以使用固定的学习率。保证SGD收敛地一个充分条件是 $$\sum\limits_{k=1}^\infty\epsilon_k=\infty$$ 且 $$\sum\limits_{k=1}^\infty\epsilon_k^2<\infty$$ 。

实践中，一般会线性衰减学习率知道第 $$\tau$$ 次迭代：

                                                                     $$\epsilon_k=(1-\alpha)\epsilon_0+\alpha\epsilon_\tau$$ 

其中 $$\alpha=\frac{k}{\tau}$$ 。在 $$\tau$$ 步迭代之后，一般使 $$\epsilon$$ 保持常数。

学习率可通过试验和误差来选取，通常最好的选择方法是监测目标函数值随时间变化的学习曲线。与其说是科学，这更像是一门艺术，我们应该谨慎地参考关于这个问题的大部分指导。使用线性策略时，需要选择的参数为 $$\epsilon_0,\ \epsilon_\tau,\ \tau$$ 。通常 $$\tau$$ 被设为需要反复遍历训练集几百次的迭代数。通常 $$\epsilon_\tau$$ 应设为大约 $$\epsilon_0$$ 的 $$1\%$$ 。主要问题是如何设置 $$\epsilon_0$$：若 $$\epsilon_0$$ 太大，学习曲线将会剧烈震荡，代价函数值通常会明显增加；温和的震荡是良好的，容易在训练随机代价函数（例如使用Dropout的代价函数）时出现；如果学习率太小，那么学习过程会很缓慢。如果初始学习率太低，那么学习可能会卡在一个相当高的代价值。通常，就总训练时间和最终代价值而言，最优初始学习率会高于大约迭代 $$100$$ 次左右后达到最佳效果的学习率。因此，通常最好是检测最早的几轮迭代，选择一个比在效果上表现最佳的学习率更大的学习率，但又不能太大导致严重的震荡。

SGD及相关的小批量亦或更广义的基于梯度优化的在线学习算法，一个重要的性质是每一步更新的计算时间不依赖训练样本数目的多寡。即使训练样本数目非常大时，它们也能收敛。对于足够大的数据集，SGD可能会在处理整个训练集之前就收敛到最终测试集误差的某个固定容差范围内。

研究优化算法的收敛率，一般会衡量额外误差\(excess error\) $$J(\theta-\min_\theta J(\theta))$$ ，即当前代价函数超过最低可能代价的量。SGD应用于凸问题时， $$k$$ 步迭代后的额外误差量级是 $$O(\frac{1}{\sqrt{k}})$$ ，在强凸情况下式 $$O(\frac{1}{k})$$ 。除非假定额外的条件，否则这些界限不能进一步改进。批量梯度下降在理论上比随机梯度下降有更好的收敛率，然而，泛化误差的下降速度不会快于 $$O(\frac{1}{k})$$ 。对于大数据集，SGD只需非常少量样本计算梯度从而实现初始快速更新，远远超过了其缓慢地渐进收敛。我们也可以在学习过程中逐渐增大小批量的大小，以此权衡批梯度下降和随机梯度下降两者的优点。

## 动量\(Momentum\)

虽然随机梯度下降仍然是非常受欢迎的优化方法，但其学习过程有时会很慢。动量方法旨在加速学习，特别是处理高曲率、小但一致的梯度，或是带噪声的梯度。动量算法积累了之前梯度指数级衰减的移动平均，并且继续沿该方向移动。

SGD很难通过陡谷，即在一个维度上的表面弯曲程度远大于其他维度的区域，这种情况通常出现在局部最优点附近。在这种情况下，SGD摇摆地通过陡谷的斜坡，同时，沿着底部到局部最优点的路径上只是缓慢地前进，这个过程如下图黑线所示。动量法是一种帮助SGD在相关方向上加速并抑制摇摆的一种方法。动量法将历史步长的更新向量的一个分量增加到当前的更新向量中。

![](../../../.gitbook/assets/timline-jie-tu-20181217120152.png)

动量的主要目的是解决两个问题：Hessian矩阵的病态条件和随机梯度的方差。我们通过上图说明动量如何客服这两个问题的第一个。等高线描绘了一个二次损失函数（具有病态条件的Hessian矩阵）。横跨轮廓的红色路径表示动量学习规则所遵循的路径，它使该函数最小化。我们在该路径的每个步骤画一个箭头，表示梯度下降将在该店采取的步骤。我们可以看到，一个病态的二次目标函数看起来想一个长而窄的山谷或具有陡峭边的峡谷。动量正确地纵向穿过峡谷，而普通的梯度步骤则会浪费时间在峡谷的窄轴上来回移动

从形式上看，动量算法引入了变量 $$v$$ 充当速度角色——它代表参数在参数空间移动的方向和速率。速度被设为负梯度的指数衰减平均。名称动量\(momentum\)来自物理类比，根据牛顿运动定律，负梯度是移动参数空间中粒子的力。动量在物理学上定义为质量乘以速度。在动量学习算法中，我们假设是单位质量，因此速度向量 $$v$$ 也可以看作是粒子的动量。超参数 $$\alpha\in[0,1)$$ 决定了之前梯度的贡献衰减得有多快。更新规则如下：

                                                    $$v = \alpha v-\epsilon\nabla_\theta(\frac{1}{m}\sum\limits_{i=1}^mL(f(x^{(i)};\theta),y^{(i)}))$$ 

                                                    $$\theta \gets \theta+v$$ 

速度 $$v$$ 积累了梯度元素 $$\nabla_\theta(\frac{1}{m}\sum\limits_{i=1}^mL(f(x^{(i)};\theta),y^{(i)}))$$ 。相对于 $$\epsilon$$ ， $$\alpha$$ 越大，之前梯度对现在方向的影响也越大。

#### 算法过程

初始化：学习率 $$\epsilon_k$$，动量参数 $$\alpha$$，初始速度$$v$$，初始参数 $$\theta$$ 

* while 停止准则为满足 do：
*            从训练集中采包含 $$m$$ 个样本 $$\{x^{(1)},\dots,x^{(m)}\}$$ 的小批量，其中 $$x^{(i)}$$ 对应目标为 $$y^{(i)}$$ 
*            计算梯度估计： $$g\gets \frac{1}{m}\nabla_\theta\sum\limits_i^m L(f(x^{(i)};\theta),y^{(i)})$$ 
*            计算速度更新： $$v\gets \alpha v-\epsilon g$$ 
*            应用更新：$$\theta \gets \theta + v$$ 

之前，步长只是梯度范数乘以学习率。现在，步长取决于梯度序列的大小和排列。当许多连续的梯度指向相同方向时，步长最大。如果动量算法总是观测到梯度 $$g$$ ，那么它会在方向 $$-g$$ 上不停加速，知道达到最终速度，其中步长大小为 $$\frac{\epsilon||g||}{1-\alpha}$$ 。因此将动量的超参数视为 $$\frac{1}{1-\alpha}$$ 有助于理解。例如， $$\alpha=0.9$$ 对应着最大速度 $$10$$ 倍于梯度下降算法。

在实践中， $$\alpha$$ 的一般取值为 $$0.5$$ ， $$0.9$$ 和 $$0.99$$ 。和学习率一样， $$\alpha$$ 也会随着时间不断调整。一般初始值是一个较小的值，随后会慢慢变大。随着时间推移调整 $$\alpha$$ 没有收缩 $$\epsilon$$ 重要。

## Nesterov加速梯度下降法\(NAG\)

Nesterov加速梯度下降法\(Nesterov accelerated gradient，NAG）

## Adagrad

## Aadadelta

## Rmsprop

## Adam

## Source

{% embed url="https://blog.csdn.net/google19890102/article/details/69942970" %}

{% embed url="http://cs231n.github.io/neural-networks-3/" %}



