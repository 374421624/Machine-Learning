# 噪声鲁棒性

一般情况下，噪声注入远比简单的收缩参数强大，特别是噪声被添加到隐藏单元时。

另一种正则化模型的噪声使用方法是将其直接添加到学习到的权重上。这项技术主要被用于循环神经网络的情况下。这种方法可以看作是权重的贝叶斯推断的一种随机实现。贝叶斯方法认为学习到的模型权重上不确定的，并且这种不确定性可以通过权重的概率分布来表示。添加噪音到学习到的权重上可以看着是反映这种不确定行的一种随机的、实用的方式。

在适当的假设下，施加噪声到权重可以被解释与传统的正则化形式等价，鼓励学习到的函数保存一定的稳定性。这种形式的正则化鼓励模型的参数进入到参数空间中相对较稳定的区域，在这些区域小的权重扰动对于模型的输出影响较小。

大多数数据集的标签 $$y$$ 都有一定错误，错误的 $$y$$ 不利于最大化 $$\log p(y|x)$$ 。 避免这种情况的一种方法是显式地对标签上的噪声进行建模。 例如，我们可以假设，对于一些小常数 $$\epsilon$$ ，训练集标记 $$y$$ 是正确的概率是 $$1-\epsilon$$ ，（以 $$\epsilon$$ 的概率）任何其他可能的标签也是正确的。这个假设很容易就能解析地与代价函数结合，而不用显式地抽取噪声样本。 例如，标签平滑\(label smoothing\) 通过把确切分类目标从 $$0$$ 和 $$1$$ 替换成 $$\frac{\epsilon}{k-1}$$ 和 $$1-\epsilon$$ ，正则化具有 $$k$$ 个输出的softmax函数模型。标准交叉熵损失可以用在这些非确切目标的输出上。 使用softmax函数和明确目标的最大似然学习可能永远不会收敛，因为softmax函数永远无法真正预测 $$0$$ 概率或 $$1$$ 概率，因此它会继续学习越来越大的权重，使预测更极端。使用如权重衰减等其他正则化策略能够防止这种情况。标签平滑的优势是能够防止模型追求确切概率而不影响模型学习正确分类。这种策略自20世纪80年代就已经被使用，并在现代神经网络继续保持显著特色。

