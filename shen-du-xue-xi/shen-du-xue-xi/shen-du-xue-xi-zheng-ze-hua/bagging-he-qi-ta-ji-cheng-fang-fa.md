# Bagging和其他集成方法

集成化方法是一种通用的降低泛化误差的方法，通过合并多个模型的结果，也叫作模型平均。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。

经验：原始输入每一个节点选择概率0.8，隐藏层选择概率为0.5。

Bagging是一种常用的集成学习方法。Bagging的策略很多，例如不同初始化方法、不同mini batch选择方法、不同的超参数选择方法。

与之对应的集成方法是Boosting，通过改变样本权重来训练不同模型。

