# 正则化和欠约束问题

在某些情况下，为了正确定义机器学习问题，正则化是必要的。机器学习中许多线性模型，包括线性回归和PCA，都依赖于对矩阵 $$X^TX$$ 求逆。只要 $$X^TX$$ 是奇异的，这些方法就会失效。当数据生成分布在一些方向上确实没有差异时，或因为例子较少（相对于输入特征维度来说）而在一些方向上没有观察到方差时，这个矩阵就是奇异的。在这种情况下，正则化的许多形式对求你 $$X^TX+\alpha I$$ 。这个正则化矩阵可以保证是可逆的。

相关矩阵可逆时，这些线性问题有闭式解。没有闭式解的问题也可能是欠定的。一个例子是应用于线性可分问题的逻辑回归。如果权重向量 $$w$$ 能够实现完美分类，那么 $$2w$$ 也会以更高似然实现完美分类。类似随机梯度下降的迭代优化算法将持续增加 $$w$$ 的大小，理论上永远不会停止。在实践中，数值实现的梯度下降最终达到导致数值溢出的超大权重，此时的行为将取决于程序员如何处理这些不是真正数字的值。

大多数形式的正则化能够保证应用于欠定问题的迭代方法收敛。例如，当似然的斜率等于权重衰减的系数时，权重衰减将阻止梯度下降继续增加权重的大小。

