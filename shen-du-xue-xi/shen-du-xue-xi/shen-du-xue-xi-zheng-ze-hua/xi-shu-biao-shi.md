# 稀疏表示

深度学习可以看着时一种表示学习（representation learning），比如卷积神经网络可以学习图像的不同层次的特征表示，word2vec学习词的Distributed representation，其共同特点是用隐层权重作为表示。

$$L^1$$ 惩罚可以诱导稀疏的参数，即许多参数为零（或接近于零）。

表示的范数惩罚正则化是通过向损失函数 $$J$$ 添加对表示的范数惩罚来实现的。 我们将这个惩罚记作 $$\Omega(h)$$ 。 和以前一样，我们将正则化后的损失函数记作 $$\tilde{J}$$ ：

                                             $$\tilde{J}(\theta;X,y)=J(\theta;X,y)+\alpha\Omega(h)$$ 

其中 $$\alpha \in [0,\infty]$$ 权衡范数惩罚项的相对贡献，越大的 $$\alpha$$ 对应越多的正则化。

通过上述方法，含有隐藏单元的模型在本质上都能变得稀疏。

