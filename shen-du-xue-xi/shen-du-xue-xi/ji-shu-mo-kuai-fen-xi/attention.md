# Attention

在循环和递归网络章节注意力机制里我们介绍了RNN中Attention的应用，这里我们深度剖析一下Attention。在CNN、FCN、RCNN等传统图像识别模型中，输入往往是一整张图，然后使用以卷积为主的模型迭代出结果。Google DeepMind的科学家指出，这种方式具有明显的不足。如果图像中要是别的目标过多、信息过大，RCNN之类模型的区域检测加识别的方式往往很慢，因为每一个ROI（Region Of Interest）都要先判定是否是候选框，如果是候选框，还要送去预测子网络做预测。尽管近年来的论文不断改进卷积模型，让大量卷积层实现共享以提升效率，但传统模型的总体效率和精度也难以再大幅提高了。

近年来对神经学和认知科学的研究表明，传统模型效率不高（准确率往往也不够高）的原因很可能在于其观察事物的方式不够好。既然我们想要解决ROI在哪里并且是什么，那么何必要在全图均匀探索信息（卷积），而不直接从一个ROI跳转到另一个呢？Attention机制据此另辟蹊径，提供了一种全新的认知图像的方式。

我们先来看看人是怎么辨别一张图上的信息的。人在看东西的时候往往会首先定位一个感兴趣的区域，这种该关注哪里就看哪里的机制就是注意力机制（Attention）。辨别出该区域的信息后，目标再移动到下一个感兴趣的区域，对于细节丰富、信息量大的地方，目光可能移动缓慢；对于背景等不重要的地方，可能只稍微看看甚至直接跳过；对于看不懂的地方，目光可能先跳过，最后再折回来（双向Attention）。目光按序扫完整张图后，人对图上的信息也就基本了解了。

Attention仿照上述观察方式，在深度学习网络中加入了感兴趣区域的移动和定位机制，使比较复杂的任务分解成相对简单的子步骤序列。每一步只关注特定小区域，抽取出区域表征的信息，再整合到之前的步骤所积累的信息中。在这样反复观察和整合中，模型逐步学懂了输入源中所蕴含的信息（甚至是层次关系），并对自己的判断也越来越有信心。同时，图片中信息分类置信度的变化，也作为一个反馈信号进一步刺激模型去学习怎样定位下一步的观测位置。

## 从简单RNN到RNN+Attention

近年来RNN模型的爆炸式发展使传统模型的整体输入（如整张图）分解为序列化的子输入（整张图的小块）变为可能。简单的RNN模型可以在积累多步的时序信息之后得出结论，但是这种迭代方式存在一定的问题。每步RNN的输出向量 $$h(t)$$ 一般只依赖于上一步的输出 $$h(t-1)$$ ，这样随着步数的增加，离 $$t$$ 时刻较远的信息会被慢慢丢失。LSTM解决了长距离信息丢失的问题，它拥有一个记忆区，通过三个门开关（输入门、遗忘门、输出门）对记忆区进行维护。每当有新的输入到来时，LSTM就将新的信息有选择地加入到记忆区里，并对以往的信息进行有选择的保留，将记忆区的新老信息整合之后再选择性输出。

## Soft Attention与Hard Attention

## Attention的应用



