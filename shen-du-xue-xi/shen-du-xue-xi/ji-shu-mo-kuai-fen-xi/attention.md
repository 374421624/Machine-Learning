# Attention

在循环和递归网络章节注意力机制里我们介绍了RNN中Attention的应用，这里我们深度剖析一下Attention。在CNN、FCN、RCNN等传统图像识别模型中，输入往往是一整张图，然后使用以卷积为主的模型迭代出结果。Google DeepMind的科学家指出，这种方式具有明显的不足。如果图像中要是别的目标过多、信息过大，RCNN之类模型的区域检测加识别的方式往往很慢，因为每一个ROI（Region Of Interest）都要先判定是否是候选框，如果是候选框，还要送去预测子网络做预测。尽管近年来的论文不断改进卷积模型，让大量卷积层实现共享以提升效率，但传统模型的总体效率和精度也难以再大幅提高了。

近年来对神经学和认知科学的研究表明，传统模型效率不高（准确率往往也不够高）的原因很可能在于其观察事物的方式不够好。既然我们想要解决ROI在哪里并且是什么，那么何必要在全图均匀探索信息（卷积），而不直接从一个ROI跳转到另一个呢？Attention机制据此另辟蹊径，提供了一种全新的认知图像的方式。

我们先来看看人是怎么辨别一张图上的信息的。人在看东西的时候往往会首先定位一个感兴趣的区域，这种该关注哪里就看哪里的机制就是注意力机制（Attention）。辨别出该区域的信息后，目标再移动到下一个感兴趣的区域，对于细节丰富、信息量大的地方，目光可能移动缓慢；对于背景等不重要的地方，可能只稍微看看甚至直接跳过；对于看不懂的地方，目光可能先跳过，最后再折回来（双向Attention）。目光按序扫完整张图后，人对图上的信息也就基本了解了。

Attention仿照上述观察方式，在深度学习网络中加入了感兴趣区域的移动和定位机制，使比较复杂的任务分解成相对简单的子步骤序列。每一步只关注特定小区域，抽取出区域表征的信息，再整合到之前的步骤所积累的信息中。在这样反复观察和整合中，模型逐步学懂了输入源中所蕴含的信息（甚至是层次关系），并对自己的判断也越来越有信心。同时，图片中信息分类置信度的变化，也作为一个反馈信号进一步刺激模型去学习怎样定位下一步的观测位置。

## Attention Mechanism

### Attention解决问题

[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)介绍了一种基于RNN的Seq2Seq模型，基于一个Encoder和一个Decoder来构建基于神经网络的End-to-End的机器翻译模型，其中，Encoder把输入 $$X$$ 编码成一个固定长度的隐向量 $$Z$$ ，Decoder基于隐向量Z解码出目标输出 $$Y$$ 。这是一个非常经典的序列到序列的模型，但是却存在两个明显的问题：

1. 把输入 $$X$$ 的所有信息有压缩到一个固定长度的隐向量 $$Z$$ ，忽略了输入输入 $$X$$ 的长度，当输入句子长度很长，特别是比训练集中最初的句子长度还长时，模型的性能急剧下降。
2. 把输入 $$X$$ 编码成一个固定的长度，对于句子中每个词都赋予相同的权重，这样做是不合理的，比如，在机器翻译里，输入的句子与输出句子之间，往往是输入一个或几个词对应于输出的一个或几个词。因此，对输入的每个词赋予相同权重，这样做没有区分度，往往是模型性能下降。

同样的问题也存在于图像识别领域，卷积神经网络CNN对输入的图像每个区域做相同的处理，这样做没有区分度，特别是当处理的图像尺寸非常大时，问题更明显。因此，2015年，Dzmitry Bahdanau等人在[Neural machine translation by jointly learning to align and translate](https://arxiv.org/abs/1409.0473)提出了Attention Mechanism，用于对输入 $$X$$ 的不同部分赋予不同的权重，进而实现软区分的目的。

### Attention原理

## 从简单RNN到RNN+Attention

近年来RNN模型的爆炸式发展使传统模型的整体输入（如整张图）分解为序列化的子输入（整张图的小块）变为可能。

简单的RNN模型可以在积累多步的时序信息之后得出结论，但是这种迭代方式存在一定的问题。每步RNN的输出向量 $$h(t)$$ 一般只依赖于上一步的输出 $$h(t-1)$$ ，这样随着步数的增加，离 $$t$$ 时刻较远的信息会被慢慢丢失。

LSTM解决了长距离信息丢失的问题，它拥有一个记忆区，通过三个门开关（输入门、遗忘门、输出门）对记忆区进行维护。每当有新的输入到来时，LSTM就将新的信息有选择地加入到记忆区里，并对以往的信息进行有选择的保留，将记忆区的新老信息整合之后再选择性输出。但是这种方法也有明显的不足，即LSTM每一步接受的输入只能是事先给定的，因此这个输入可能是冗余信息，甚至是对当前 $$t$$ 时刻不恰当的信息。

基于Attention的模型则强调从之前输入已观察到的结果（从输入中观察到的信息）和下一步如何部署观察位置之间的互动，其采用的Attention机制就是根据已观察到的环境信息从众多备选输入中选择“最合适”的输入。当 $$t$$ 时刻输入 $$z(t)$$ 到来时，会得到一个输出向量 $$h(t)$$ ，同时RNN的内部信息状态也将被改变。输出向量 $$h(t)$$ ，可以代表模型当前大脑中存储的对环境信息的表征（Encoded Vector），此时将反过来作为Attention模块的输入，用以计算出下一步RNN的模型的输入 $$z(t+1)$$ 。

## Attention Mechanism分类

### Soft Attention和Hard Attention

Attention模块所进行的选择过程可以表现为某种函数映射 $$f$$ ，具体实现分为两种形式：Soft Attention和Hard Attention。如果所有候选都以一定的概率参与选择，即结果为所有候选向量的概率加权，则称为Soft Attention；如果只从候选中选取某一个候选向量，则称为Hard Attention。

比如图片经过多次卷积后得到一个网格，其中 $$a ,b,c,d$$ 对应于经过卷积之后从4个位置抽取出的候选向量。那么如何通过映射函数 $$f$$ 得到下一步输入到RNN中的向量 $$z(t+1)$$ 呢？首先，Attention模块会在当前步根据RNN的环境变量 $$h(t)$$ 算出一个候选向量的概率分布。对于Soft Attention，求 $$z$$ 需要将每个候选向量进行概率加权，即 $$z=p_aa+p_bb+p_cc+p_dd$$ ；Hard Attention则仅通过概率分布采样出一个候选向量赋值给 $$z(t+1)$$ 。

| 策略 | Soft Attention | Hard Attention |
| :---: | :---: | :---: |
| 筛选范围 | 全部 | 一个 |
| 计算方式 | 概率加权 | 采样 |
| 是否可导 | 可导 | 不可导 |
| 训练方式 | 梯度下降 | 强化学习 |

### Global Attention和Local Attention

### Self Attention

## 组合的Attention结构

## Attention应用场景

### **机器翻译（Machine Translation）**

### **图像标注（Image Captain）**

### **关系抽取（EntailMent Extraction）**

### **语音识别（Speech Recognition）**

### **自动摘要生成（Text Summarization）**

## Source

{% embed url="https://zhuanlan.zhihu.com/p/31547842" %}

