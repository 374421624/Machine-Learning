# 未知环境模型的问题

对于绝大多数现实中的增强学习问题，Agent只能在采取动作之后获得下一个状态以及对应的奖励，对其中的概率转移函数和奖励函数的细节无法知晓。这一类算法也被称为无模型（Model-Free）算法。当然，没有模型不代表无法进行求解，我们可以通过采样的方法进行求解，也就是具有增强学习特色的学习方式——在实践中学习，当模型通过实践采样到大量的与环境交互的数据之后，它就可以利用这些数据求出价值函数的模型，求解在当前数据下的最优策略了。在无模型算法中，比较经典的有蒙特卡罗法和时序差分法。

## 蒙特卡罗法

在很多增强学习问题中，我们发现想获得环境模型完整的状态转移概率比较困难，但是获取某一步动作后的状态和奖励序列是比较容易的。比较典型的例子就是电子游戏，一般来说，玩家想要把每一个场景（也可以称作状态）下经过任意操作后产生的状态完全收集到是不太现实的，但是游戏会自然而然地为玩家展现出它的下一个场景和奖励。这里通常只考虑有明确终止状态的问题，这样将每一轮Agent从开始到结束的MDP（马尔可夫决策过程）称为一个交互流（Episode）。通过和环境的不断交互，Agent可以获得一定数量的交互流，并利用这些交流学习策略。实际上蒙特卡罗法的总体计算思路和动态规划法的计算思路相似。对于蒙特卡罗法，首先要做的同样是策略估计，也就是对于固定的策略 $$\pi$$ ，计算 $$v_\pi$$ 和 $$q_\pi$$ ，然后再进行策略改进。唯一不同的是，蒙特卡罗法在策略评估这一步采用了蒙特卡罗采样的方法。蒙特卡罗法的核心思想就是将交互流中顺序出现的状态和动作进行平均，从而得到对于每一个状态的价值函数，然后利用这个价值函数求出相应的策略。蒙特卡罗法在统计样本的过程中有两种方式：

* 全样本统计法（Every-visit MC method），也就是把每一个交互流中出现的所有状态对都记录下来，用于评估状态的价值函数。
* 首次出现样本统计法（First-visit MC method），也就是对于每一个状态 $$s$$ ，在每一个交互流中只统计第一次出现的状态信息，用于评估状态的价值函数。

以下是蒙特卡罗版的策略评估算法，这里采用首次出现样本统计法。

初始化： $$\pi\gets$$ 需要评估的策略， $$V\gets$$ 任意状态-值函数， $$\text{Returns}(s)\gets$$ 空列表，针对所有 $$s\in S$$ 

Repeat forever：

* （a）使用策略 $$\pi$$ 生成一个交互流
* （b）针对交互流中出现的每一个状态 $$s$$ ：
*                   $$G\gets s$$ 第一次出现时的回报
*                   将 $$G$$ 添加在列表 $$\text{Returns}(s)$$ 的最后
*                   $$V(s)\gets \text{average(Return(s))}$$ 

上面的算法评估是状态价值函数，在拥有模型详细信息的情况下，利用

## 时序差分法

### Q-Learning

