# 未知环境模型的问题

对于绝大多数现实中的增强学习问题，Agent只能在采取动作之后获得下一个状态以及对应的奖励，对其中的概率转移函数和奖励函数的细节无法知晓。这一类算法也被称为无模型（Model-Free）算法。当然，没有模型不代表无法进行求解，我们可以通过采样的方法进行求解，也就是具有增强学习特色的学习方式——在实践中学习，当模型通过实践采样到大量的与环境交互的数据之后，它就可以利用这些数据求出价值函数的模型，求解在当前数据下的最优策略了。在无模型算法中，比较经典的有蒙特卡罗法和时序差分法。

## 蒙特卡罗法

在很多增强学习问题中，我们发现想获得环境模型完整的状态转移概率比较困难，但是获取某一步动作后的状态和奖励序列是比较容易的。比较典型的例子就是电子游戏，一般来说，玩家想要把每一个场景（也可以称作状态）下经过任意操作后产生的状态完全收集到是不太现实的，但是游戏会自然而然地为玩家展现出它的下一个场景和奖励。这里通常只考虑有明确终止状态的问题，这样将每一轮Agent从开始到结束的MDP（马尔可夫决策过程）称为一个交互流（Episode）。通过和环境的不断交互，Agent可以获得一定数量的交互流，并利用这些交流学习策略。实际上蒙特卡罗法的总体计算思路和动态规划法的计算思路相似。对于蒙特卡罗法，首先要做的同样是策略估计，也就是对于固定的策略 $$\pi$$ ，计算 $$v_\pi$$ 和 $$q_\pi$$ ，然后再进行策略改进。唯一不同的是，蒙特卡罗法在策略评估这一步采用了蒙特卡罗采样的方法。蒙特卡罗法的核心思想就是将交互流中顺序出现的状态和动作进行平均，从而得到对于每一个状态的价值函数，然后利用这个价值函数求出相应的策略。蒙特卡罗法在统计样本的过程中有两种方式：

* 全样本统计法（Every-visit MC method），也就是把每一个交互流中出现的所有状态对都记录下来，用于评估状态的价值函数。
* 首次出现样本统计法（First-visit MC method），也就是对于每一个状态 $$s$$ ，在每一个交互流中只统计第一次出现的状态信息，用于评估状态的价值函数。

以下是蒙特卡罗版的策略评估算法，这里采用首次出现样本统计法。

初始化： $$\pi\gets$$ 需要评估的策略， $$V\gets$$ 任意状态-值函数， $$\text{Returns}(s)\gets$$ 空列表，针对所有 $$s\in S$$ 

Repeat forever：

* （a）使用策略 $$\pi$$ 生成一个交互流
* （b）针对交互流中出现的每一个状态 $$s$$ ：
*                   $$G\gets s$$ 第一次出现时的回报
*                   将 $$G$$ 添加在列表 $$\text{Returns}(s)$$ 的最后
*                   $$V(s)\gets \text{average(Return(s))}$$ 

上面的算法评估是状态价值函数，在拥有模型详细信息的情况下，利用它推导出策略是没有问题的；但是在没有模型信息的情况下，计算策略需要从每一个状态出发，遍历所有的动作计算出 $$q(s,a)$$ ，然后再求出最优策略 $$\arg\max q(s,a)$$ 。既然这样，我们需要考虑评估 $$q(s,a)$$ 的蒙特卡罗法。同样可以采用评估 $$v(s)$$ 的方法来评估 $$q(s,a)$$ ，但是这里同样有一个问题，那就是有的 $$(s,a)$$ 对可能极难出现在交流中。对于没有出现的 $$(s,a)$$ 对来说，计算他们的价值比较困难，因此判断这个动作是否是这个 $$s$$ 的最优策略变得比较困难。对于这个问题，一种解决方法就是保持探索（Exploration），给所有没有在交互流中出现的动作一定的概率。在完成评估后，我们就可以求出当前价值函数下的策略，也就是策略改进的步骤。完整的算法如下：

初始化：

* for all $$s\in S, a\in A(s)$$ ：
*         $$Q(s,a)\gets$$ 任意值
*         $$\pi(s)\gets$$ 任意值
*          $$\text{Returns}(s,a)\gets$$ 空列表

Repeat forever：

* （a）选择 $$S_0\in S$$ 和 $$A_0\in A(s_0)$$ ，这样可以保证所有的pair概率都大于0
* （b）根据策略 $$\pi$$ 生成一个交互流
* （c）针对出现在交互流中的每一个 $$\langle s,a\rangle$$ 对
*                 $$G\gets\langle s,a\rangle$$ 第一次出现时对应的回报
*                  将 $$G$$ 添加在列表 $$\text{Returns}(s)$$ 的最后
*                 $$Q(s,a)\gets \text{average(Return(s,a))}$$ 
* （d）针对交互流中的每一个状态 $$s$$ 
*                  $$\pi(s)\gets \arg\max_aQ(s,a)$$ 

## 时序差分法

时序差分法（Temporal Difference）结合了动态规划法和蒙特卡罗法的特点。和动态规划法类似，时序差分法的参数更新只需要交互流中的部分信息即可，不需要像蒙特卡罗法那样收集完一个交互流之后才能进行计算；同时和蒙特卡罗法类似，时序差分法同样根据交互流的信息进行参数更新，而不像动态规划法那样需要知道模型的细节信息。在蒙特卡罗法中，需要等待一个交流结束后，计算在这个交互流中某个状态 $$s$$ 的回报 $$G_t$$ ，然后用这个回报更新状态 $$s$$ 的价值函数，公式如下：

                                                     $$V(S_t)\gets V(S_t)+\alpha[G_t-V(S_t)]$$ 

式中的 $$\alpha$$ 指的是更新步长。与蒙特卡罗法不同的是，时序差分法只需要等待下一个状态产生即可。利用当前价值函数对下一个状态的评估 $$V(S_{t+1})$$ 和动作产生的奖励 $$R_{t+1}$$ ，就可以组合得到状态 $$S_t$$ 的回报。于是可以得到如下公式：

                                        $$V(S_t)\gets V(S_t)+\alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]$$ 

这就是时序差分法的更新公式。从另一个角度考虑这个公式，可以将公式进行转换，得到：

                                         $$V(S_t)\gets (1-\alpha)V(S_t)+\alpha[R_{t+1}+\gamma V(S_{t+1})]$$ 

实际上 $$\alpha$$ 相当于一个滑动平均的参数，让最新的价值函数值由原有的价值函数值和计算得到的回报加权平均得到。以下是时序差分法的策略评估过程。

输入：需要评估的策略 $$\pi$$ 

随机初始化 $$V(s)$$ （例如： $$V(s)=0,\forall s\in S^+$$ ）

Repeat \(for each episode\)：

           初始化 $$S$$ 

           Repeat \(for each step of episode\)：

                    $$A\gets$$ 策略 $$\pi$$ 在 $$S$$ 时采用的动作

                    采取动作 $$A$$ ，得到奖励 $$R$$ 及下一状态 $$S'$$ 

                    $$V(S)\gets V(S)+\alpha[R+\gamma V(S')-V(S)]$$ 

                     $$S\gets S'$$ 

            until $$S$$ 是结束状态

时序差分法和蒙特卡罗法相比有一定的优势，它可以自然地实现为在线增量的算法，不必等到整个交互流结束后才更新价值函数，而且在很多实际应用中，时序差分法比蒙特卡罗法拥有更快的收敛速度。

### Q-Learning



